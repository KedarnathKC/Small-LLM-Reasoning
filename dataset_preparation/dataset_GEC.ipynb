{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14826356-d9bd-4b79-838c-85a82a59b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_m2(m2_path):\n",
    "#     \"\"\"\n",
    "#     Parse an M2 file into a list of dicts, each with:\n",
    "#       - 'src':   the original (erroneous) sentence\n",
    "#       - 'edits': list of edits, each dict:\n",
    "#           - 'start': int character offset\n",
    "#           - 'end':   int character offset\n",
    "#           - 'etype': str error type\n",
    "#           - 'corr':  str replacement text\n",
    "#     \"\"\"\n",
    "#     examples = []\n",
    "#     with open(m2_path, encoding='utf-8') as f:\n",
    "#         src = None\n",
    "#         edits = []\n",
    "#         for line in f:\n",
    "#             line = line.rstrip(\"\\n\")\n",
    "#             # blank line → end of this example\n",
    "#             if not line:\n",
    "#                 if src is not None:\n",
    "#                     examples.append({\"input\": src, \"edits\": edits})\n",
    "#                 src, edits = None, []\n",
    "#                 continue\n",
    "\n",
    "#             if line.startswith(\"S \"):\n",
    "#                 # Source sentence\n",
    "#                 src = line[2:]\n",
    "#             elif line.startswith(\"A \"):\n",
    "#                 # Annotation line → strip off the \"A \"\n",
    "#                 ann = line[2:]\n",
    "#                 parts = ann.split(\"|||\")\n",
    "#                 # parts[0] is \"start end\"\n",
    "#                 offsets = parts[0].split()\n",
    "#                 if len(offsets) != 2:\n",
    "#                     # malformed, skip\n",
    "#                     continue\n",
    "#                 start, end = int(offsets[0]), int(offsets[1])\n",
    "\n",
    "#                 etype = parts[1]\n",
    "#                 corr  = parts[2]\n",
    "\n",
    "#                 # Skip noop edits (no change)\n",
    "#                 if etype.lower() == \"noop\" or (start == -1 and end == -1):\n",
    "#                     continue\n",
    "\n",
    "#                 edits.append({\n",
    "#                     \"start\": start,\n",
    "#                     \"end\":   end,\n",
    "#                     \"etype\": etype,\n",
    "#                     \"corr\":  corr\n",
    "#                 })\n",
    "#         # handle case where file doesn't end with a blank line\n",
    "#         if src is not None:\n",
    "#             examples.append({\"input\": src, \"edits\": edits})\n",
    "\n",
    "#     return examples\n",
    "\n",
    "\n",
    "# def apply_edits_to_sentence(src, edits):\n",
    "#     \"\"\"\n",
    "#     Apply token-level edits to a tokenized source string.\n",
    "\n",
    "#     Args:\n",
    "#       src   (str): whitespace-tokenized sentence\n",
    "#       edits (list of dict): each with 'start','end','corr'\n",
    "\n",
    "#     Returns:\n",
    "#       str: the corrected sentence (still whitespace-tokenized)\n",
    "#     \"\"\"\n",
    "#     # 1) split into tokens & build char-span map\n",
    "#     tokens = src.split()\n",
    "#     spans = []\n",
    "#     cursor = 0\n",
    "#     for tok in tokens:\n",
    "#         # find this token at or after cursor\n",
    "#         pos = src.find(tok, cursor)\n",
    "#         if pos < 0:\n",
    "#             raise ValueError(f\"Token {tok!r} not found in src starting at {cursor}\")\n",
    "#         spans.append((pos, pos + len(tok)))\n",
    "#         cursor = pos + len(tok)\n",
    "\n",
    "#     corrected = src\n",
    "#     # 2) apply edits in descending token-index order\n",
    "#     for e in sorted(edits, key=lambda x: x[\"start\"], reverse=True):\n",
    "#         st, en, corr = e[\"start\"], e[\"end\"], e[\"corr\"]\n",
    "#         # compute char positions\n",
    "#         if st < len(spans):\n",
    "#             char_start = spans[st][0]\n",
    "#         else:\n",
    "#             # insertion at end of sentence\n",
    "#             char_start = len(corrected)\n",
    "\n",
    "#         if en == 0:\n",
    "#             # insertion before 1st token\n",
    "#             char_end = 0\n",
    "#         elif en <= len(spans):\n",
    "#             # normal span; end is exclusive token index → end at end-of-token(en-1)\n",
    "#             char_end = spans[en - 1][1]\n",
    "#         else:\n",
    "#             # deletion spanning past last token\n",
    "#             char_end = len(corrected)\n",
    "\n",
    "#         # splice it in\n",
    "#         corrected = corrected[:char_start] + corr + corrected[char_end:]\n",
    "\n",
    "#     return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fabda-54ee-4fdb-b842-e8be1b08b85e",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f5ac0f5-0111-4492-a6f1-bb4c1de17de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# train_m2_file = \"ABC.train.gold.bea19.m2\"\n",
    "# examples = parse_m2(train_m2_file)\n",
    "\n",
    "# examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6dd0cf1a-d777-484f-9c78-f8a4a44d2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build a list of corrected sentences\n",
    "# corrected_sentences = []\n",
    "# for i,ex in enumerate(examples):\n",
    "#     examples[i]['id']=i+1\n",
    "#     src  = ex[\"input\"]\n",
    "#     corr = apply_edits_to_sentence(src, ex[\"edits\"])\n",
    "#     examples[i]['output']=corr\n",
    "#     corrected_sentences.append(corr)\n",
    "\n",
    "# # print them out\n",
    "# for i,o in enumerate(corrected_sentences):\n",
    "#     print(examples[i]['input'])\n",
    "#     for j,edit in enumerate(examples[i]['edits']):\n",
    "#         print(edit)\n",
    "#     print(o)\n",
    "#     print()\n",
    "#     if i>5:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91f4d102-d7f7-4490-aab9-db5ff6b49bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6333482-f140-40de-b7bf-a3e1192c247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset= Dataset.from_list(examples)\n",
    "# train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b79ec-40f5-4aaf-b386-6ba56eba943d",
   "metadata": {},
   "source": [
    "### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b72473d1-41bc-4d48-9f56-82f37600496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_m2_file='ABCN.dev.gold.bea19.m2'\n",
    "# examples= parse_m2(val_m2_file)\n",
    "# len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc32c404-c89b-4b19-b4ea-5cdd8b40aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build a list of corrected sentences\n",
    "# corrected_sentences = []\n",
    "# for i,ex in enumerate(examples):\n",
    "#     examples[i]['id']=i+1\n",
    "#     src  = ex[\"input\"]\n",
    "#     corr = apply_edits_to_sentence(src, ex[\"edits\"])\n",
    "#     examples[i]['output']=corr\n",
    "#     corrected_sentences.append(corr)\n",
    "\n",
    "# # print them out\n",
    "# for i,o in enumerate(corrected_sentences):\n",
    "#     print(examples[i]['input'])\n",
    "#     for j,edit in enumerate(examples[i]['edits']):\n",
    "#         print(edit)\n",
    "#     print(o)\n",
    "#     print()\n",
    "#     if i>5:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1f46985-26f3-4bbb-bf93-b6edc7757f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset= Dataset.from_list(examples)\n",
    "# val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626762e-a9f7-4ab3-8a07-f5e1b7c7cd5a",
   "metadata": {},
   "source": [
    "#### USED CODE FROM:\n",
    "\n",
    "https://www.cl.cam.ac.uk/research/nl/bea2019st/#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9acb1b2a-5548-45b6-be48-b74f3027b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Apply the edits of a single annotator to generate the corrected sentences.\n",
    "def get_train_val(args):\n",
    "    m2 = open(args['m2_file']).read().strip().split(\"\\n\\n\")\n",
    "    # out = open(args.out, \"w\")\n",
    "    # Do not apply edits with these error types\n",
    "    skip = {\"noop\", \"UNK\", \"Um\"}\n",
    "    inputs=[]\n",
    "    output=[]\n",
    "    edits=[]\n",
    "    for sent in m2:\n",
    "        sent = sent.split(\"\\n\")\n",
    "        \n",
    "        cor_sent = sent[0].split()[1:] # Ignore \"S \"\n",
    "        # inputs.append(' '.join(cor_sent))\n",
    "        inputs.append(sent[0][2:])\n",
    "        e = sent[1:]\n",
    "        edits.append(e)\n",
    "        offset = 0\n",
    "        for edit in e:\n",
    "            edit = edit.split(\"|||\")\n",
    "            if edit[1] in skip: continue # Ignore certain edits\n",
    "            coder = int(edit[-1])\n",
    "            if coder != args['id']: \n",
    "                print('different coder')\n",
    "                continue # Ignore other coders\n",
    "            span = edit[0].split()[1:] # Ignore \"A \"\n",
    "            start = int(span[0])\n",
    "            end = int(span[1])\n",
    "            cor = edit[2].split()\n",
    "            cor_sent[start+offset:end+offset] = cor\n",
    "            offset = offset-(end-start)+len(cor)\n",
    "        output.append([\" \".join(cor_sent)])\n",
    "        # out.write(\" \".join(cor_sent)+\"\\n\")\n",
    "    return inputs,edits,output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082131c-815d-4936-b526-ace166ae02cb",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371d893d-c6dc-4be3-a510-5957d3006e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0499d67-a8f7-4938-aa35-63d307a88745",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "args={\n",
    "    'm2_file':'ABC.train.gold.bea19.m2',\n",
    "    'id':0\n",
    "}\n",
    "inputs,edits,output=get_train_val(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16518f4-d3c6-4433-ad22-1fbd835cea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples={\n",
    "    'id':[str(i) for i in range(len(inputs))],\n",
    "    'input':inputs,\n",
    "    'edits':edits,\n",
    "    'output':output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e393e5-9c24-4a96-b518-d317a66927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 34308\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=Dataset.from_dict(examples)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7e1771-701b-408d-9692-a5acdf502e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'input': 'My town is a medium size city with eighty thousand inhabitants .',\n",
       " 'edits': ['A 5 6|||R:OTHER|||- sized|||REQUIRED|||-NONE-|||0'],\n",
       " 'output': ['My town is a medium - sized city with eighty thousand inhabitants .']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4917c340-5f12-4255-94a3-89b666dd6f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1222ca067e0b4eddbbcd3f058df546ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/34308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34308\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "filtered = train.filter(lambda x: bool(x['output']))\n",
    "print(len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18746911-4283-4e42-a01f-1c329edeb280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 34308\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650f814-c217-4407-8cb6-1cd2fa8f4239",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f92d94e-4a53-4a62-a437-7431aed58cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_test(m2_file):\n",
    "    # Read and split on blank lines\n",
    "    with open(m2_file, encoding=\"utf-8\") as f:\n",
    "        blocks = f.read().strip().split(\"\\n\\n\")\n",
    "\n",
    "    # Error types to ignore\n",
    "    skip = {\"noop\", \"UNK\", \"Um\"}\n",
    "\n",
    "    inputs = []\n",
    "    edits = []\n",
    "    output = []\n",
    "\n",
    "    # First pass: parse inputs & edits for each block\n",
    "    for block in blocks:\n",
    "        lines = block.split(\"\\n\")\n",
    "        # original sentence (drop leading \"S \")\n",
    "        orig = lines[0][2:]\n",
    "        inputs.append(orig)\n",
    "\n",
    "        # raw edit triples/quintuples\n",
    "        this_edits = [line.split(\"|||\") for line in lines[1:]]\n",
    "        edits.append(this_edits)\n",
    "\n",
    "    # For each sentence, build one list of corrected forms (one per coder)\n",
    "    i=0\n",
    "    for orig, this_edits in zip(inputs, edits):\n",
    "        # print(f'i: {i}')\n",
    "        # print(f'This edit: {this_edits}')\n",
    "        tokens = orig.split()\n",
    "        # find all coder IDs with at least one non-skipped edit here\n",
    "        coder_ids = sorted({\n",
    "            int(ed[-1])\n",
    "            for ed in this_edits\n",
    "        })\n",
    "        per_sentence = []\n",
    "        if not coder_ids:\n",
    "            # print(f'No Coders found')\n",
    "            per_sentence=[orig]\n",
    "        # print(coder_ids)\n",
    "        f=False\n",
    "        \n",
    "        for cid in coder_ids:\n",
    "            # f=False\n",
    "            cor_sent = tokens.copy()\n",
    "            # print(f'cid: {cid}')\n",
    "            offset = 0\n",
    "            for span_str, err_type, corr_str, *rest, coder_str in this_edits:\n",
    "                if err_type in skip or int(coder_str) != cid:\n",
    "                    # print('Encountered')\n",
    "                    # print(f)\n",
    "                    # f=True\n",
    "                    continue\n",
    "                # parse \"A start end\"\n",
    "                _, s, e = span_str.split()\n",
    "                start, end = int(s), int(e)\n",
    "                cor_toks = corr_str.split()\n",
    "\n",
    "                # apply with offset\n",
    "                cor_sent[start+offset : end+offset] = cor_toks\n",
    "                offset += len(cor_toks) - (end - start)\n",
    "            \n",
    "            per_sentence.append(\" \".join(cor_sent))\n",
    "            # print(f'per_sentence: {per_sentence}')\n",
    "            # if f:\n",
    "            #     print('Breaking')\n",
    "            #     break\n",
    "        \n",
    "        i+=1\n",
    "        output.append(per_sentence)\n",
    "        # print(f'Output: {output}')\n",
    "        # if i>8:\n",
    "        #     break\n",
    "        # if f:\n",
    "        #     print(per_sentence)\n",
    "        #     break\n",
    "\n",
    "    return inputs, edits, output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ffe636-2c4b-442e-956f-e7d25e4be526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs, edits, output= get_test('test.m2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588b9325-1677-42ab-951a-96668ada4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples={\n",
    "    'id':[str(i) for i in range(len(inputs))],\n",
    "    'input':inputs,\n",
    "    'edits':edits,\n",
    "    'output':output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a2774c-64a3-44d7-82e4-ffe348a5c13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 1312\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=Dataset.from_dict(examples)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88837669-dbbe-4ef6-a93f-26cbe83d3a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff34d3d91f54282add8515e87cc94af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312\n"
     ]
    }
   ],
   "source": [
    "filtered = test.filter(lambda x: bool(x['output']))\n",
    "print(len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d730698b-871d-4e7e-bc85-8172072ed35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 1312\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380938d-8c72-4320-a3c2-672fbe663178",
   "metadata": {},
   "source": [
    "### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d4f74c4-71d3-4e15-b1e9-b80749356d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'm2_file':'ABCN.dev.gold.bea19.m2',\n",
    "    'out':'corrected',\n",
    "    'id':0\n",
    "}\n",
    "inputs,edits,output=get_train_val(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea8fc125-1921-49fa-8470-e2da19b5cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples={\n",
    "    'id':[str(i) for i in range(len(inputs))],\n",
    "    'input':inputs,\n",
    "    'edits':edits,\n",
    "    'output':output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b20e022f-bd23-4f71-9cdd-2bfff88998dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 4384\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val=Dataset.from_dict(examples)\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f82600de-8d04-4c47-b826-8c2b129f95af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc088d8de0245708685955a600d03ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4384\n"
     ]
    }
   ],
   "source": [
    "filtered = val.filter(lambda x: bool(x['output']))\n",
    "print(len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c19aa7-2c63-46df-b347-3c4e85dd391d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 4384\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26259e-4543-4afd-afe7-a24929af73be",
   "metadata": {},
   "source": [
    "### Save the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab14add2-f1c6-4583-9bfe-812d79b9a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path='/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/Small-LLM-Reasoning/datasets/gec/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "969d6e47-d5e6-4e1e-93be-58de0e33d5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde80155526447aabf088cfb128b964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split='val'\n",
    "val.save_to_disk(dir_path+split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84d0fd5a-cfb1-4f7e-9605-d3938dbe7447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1578e59d0c417a856b83d930f54a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/34308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split='train'\n",
    "train.save_to_disk(dir_path+split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "479a1af5-9b34-4ec9-bc2e-9bd9a61df3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4226c87b0f8a47b0ac2402af699de8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split='test'\n",
    "test.save_to_disk(dir_path+split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83728602-cdaa-46dd-b0c8-1e1000dad875",
   "metadata": {},
   "source": [
    "### Spliting train set into feedback and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dad4d880-f75e-42b5-834d-20de05d86ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 34308\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=load_from_disk('../datasets/gec/train/')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c13e90f-774a-44f8-85b4-0a9fc531317c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_prompt': 'Correct grammatical errors in the text by first providing a response, followed by an explanation. Please use this template for the explanation: \"The word X should be deleted/inserted/replaced by Y because ...\"',\n",
       " 'task_prompt1': 'Given an input text, the goal is to detect and correct grammatical errors in the text. First explain your reasoning by describing the grammatical errors and how to fix them and then, provide the corrected text.\\n\\nYour response should end with \"The corrected text is: [answer]\" where [answer] is the grammatically correct version of the input text.',\n",
       " 'system_msg': '',\n",
       " 'user_msg': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{question}\\n\\n### Response:\\n',\n",
       " 'assistant_msg': '{rationale} The corrected text is {response}\\n',\n",
       " 'few_shot': [{'id': '8778',\n",
       "   'input': 'way to move from place to another .',\n",
       "   'reference': 'way to move from one place to another .',\n",
       "   'rationale': 'The word \"one\" should be inserted before \"place\" because the phrase \"from place to another\" is ungrammatical. The correct idiomatic expression in English is \"from one place to another,\" which properly matches the singular \"another\" with the singular \"one place\".'},\n",
       "  {'id': '21174',\n",
       "   'input': 'Hi Alison !',\n",
       "   'reference': 'Hi Alison !',\n",
       "   'rationale': 'No grammatical errors are present in the sentence. It is already correct as an informal greeting.'},\n",
       "  {'id': '17866',\n",
       "   'input': 'So , could you tell me what is the best way to reach your house ?',\n",
       "   'reference': 'So , could you tell me which is the best way to reach your house ?',\n",
       "   'rationale': 'The word \"what\" should be replaced by \"which\" because \"which\" is used when selecting from a known or limited set of options, making it more appropriate in this context where the speaker is likely referring to specific possible ways to reach the house.'},\n",
       "  {'id': '19539',\n",
       "   'input': \"What I 'm doing is challenging the way we approach to it .\",\n",
       "   'reference': \"What I 'm doing is challenging the way we approach  it .\",\n",
       "   'rationale': 'The word to should be deleted because the verb approach is a transitive verb and does not require a preposition before its object. Including \"to\" is grammatically incorrect in this context.'},\n",
       "  {'id': '25488',\n",
       "   'input': 'The absence of the parents or the fact they usually have no control nor way to drive their choices makes me very worried about the future of our society .',\n",
       "   'reference': 'The absence of  parents or the fact they usually have no control or way to drive their choices makes me very worried about the future of our society .',\n",
       "   'rationale': 'The word the should be deleted because in this context, \"parents\" is used in a general, plural sense, and does not require the definite article.The word nor should be replaced by or because \"nor\" is typically used in negative constructions following \"neither,\" but here the sentence uses \"no control\" as a standalone negative, making \"or\" the appropriate coordinating conjunction.'}]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "task='gec'\n",
    "task_prompt_path=f'../prompts/{task}.json'\n",
    "with open(task_prompt_path) as fp:\n",
    "    task_prompt = json.load(fp)\n",
    "task_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b9b0614-d601-49ee-abc2-a55abd379cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8778', '21174', '17866', '19539', '25488']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_ids=[]\n",
    "for i in task_prompt['few_shot']:\n",
    "    few_shot_ids.append(i['id'])\n",
    "few_shot_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41c53e61-2932-4fd9-a134-e94fb861a606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 34303\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.filter(lambda row:row['id'] not in few_shot_ids)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df003e66-7007-478e-b49e-ce7a900ffe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "feedback=train.shuffle(42).select(range(400))\n",
    "feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf7c3ecd-5c66-4969-8a13-4556b0e396f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9598f425c4465a9df73f7bdf7a4403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feedback.save_to_disk('../datasets/gec/feedback-400/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8b94ccd-c8d4-4f4c-8940-195394dc51ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "feedback=train.shuffle(42).select(range(100))\n",
    "feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4db17dfa-f34f-4de7-8a57-78c8cf7e7976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e16daf0de06411399d9476bad8c37b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feedback.save_to_disk('../datasets/gec/feedback-100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ccf5b0a8-d9d3-4646-90f5-52abab59e1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 1600\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback=train.shuffle(42).select(range(1600))\n",
    "feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78de68c0-f1e9-4afa-b239-5a66b6d59293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7947cb051eda4011aaeed2ac4eab0597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feedback.save_to_disk('../datasets/gec/feedback-1600/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ce2d4-99bc-4069-97eb-32080cf64315",
   "metadata": {},
   "source": [
    "### Creating m2 files for feedback set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba5ddffe-4f89-4228-a772-b5d3d92bec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split='feedback-400'\n",
    "data=load_from_disk(f'../datasets/gec/{split}/')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17570e7e-d9a2-49fb-8d8f-bf0a0f15e5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '33345',\n",
       " 'input': 'In the first conic part it is possible to feed with the fuel ( i.e. the pellet ) that leads to the combustion chamber where the pellet will burn to generate the heat necessary to warm up the air .',\n",
       " 'edits': ['A 3 4|||R:MORPH|||conical|||REQUIRED|||-NONE-|||0',\n",
       "  'A 5 5|||M:PUNCT|||,|||REQUIRED|||-NONE-|||0',\n",
       "  'A 10 11|||R:PREP|||in|||REQUIRED|||-NONE-|||0'],\n",
       " 'output': ['In the first conical part , it is possible to feed in the fuel ( i.e. the pellet ) that leads to the combustion chamber where the pellet will burn to generate the heat necessary to warm up the air .']}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53f08881-8630-409c-91a3-13d71a044772",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "with open(f'../datasets/gec/{split}/{split}.m2', 'w', encoding='utf-8') as fout:\n",
    "        for ex in data:\n",
    "            cnt+=1\n",
    "            # 1) write the source line\n",
    "            #    M2 uses tokenized input; if your source is untokenized,\n",
    "            #    you may need to tokenize (e.g. simple .split()).\n",
    "            fout.write(\"S \" + ex['input'].strip() + \"\\n\")\n",
    "            # 2) write each edit line\n",
    "            for edit in ex['edits']:\n",
    "                fout.write(edit.strip() + \"\\n\")\n",
    "            # 3) blank line between examples\n",
    "            fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "735c9b4b-8c4f-4022-81dc-c682437ed3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4435d-e711-43f3-af45-f6494111168e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
