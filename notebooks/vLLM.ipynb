{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e98810f-66cc-4bdf-8d0c-25bcee1ca525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-05 21:47:17,832\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from vllm.inputs import TokensPrompt\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "from small_llm_reasoning.evaluation.gsm8k import get_score, eight_shot_messages\n",
    "# from small_llm_reasoning.generation.vllm_generation import llama_forward\n",
    "\n",
    "\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5664f354-d52f-4760-b461-1a8ead0c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 5473\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data_path= \"../datasets/gsm8k\"\n",
    "data = load_from_disk(f'{data_path}/train/')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b8ec1c-1fd8-4c54-9126-48c2a920ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "\n",
    "# model_name= \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name= \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84191bfa-4a79-4bed-8309-d4fc613246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(ex, few_shot):\n",
    "    \n",
    "    prompt = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Given the following problem, reason and give a final answer to the problem.\\nProblem: {ex}\\nYour response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\\n'\n",
    "        }\n",
    "    ]\n",
    "    if few_shot:\n",
    "        prompt = eight_shot_messages + prompt\n",
    "    return prompt\n",
    "\n",
    "def tokenize_function(example,few_shot):\n",
    "    prompt= get_prompt(example['question'], few_shot)\n",
    "    prompt= tokenizer.apply_chat_template(prompt,  tokenize= False, add_generation_prompt=True)\n",
    "    return {'input_ids': {'prompt_token_ids':tokenizer(prompt, add_special_tokens=False)['input_ids']}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ed9cc9-ef07-4e4c-b667-fb843014feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5473/5473 [00:03<00:00, 1443.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenized_dataset = data.map(tokenize_function, batched=False)\n",
    "tokenized_dataset = data.map(lambda x: tokenize_function(x,few_shot=False), batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089f23c9-7a4c-482e-9b25-eb0a3aab99cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5473"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0f7542-95f3-4b68-b224-feb4edd23642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5473/5473 [00:00<00:00, 147333.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(f\"{data_path}/tokenized/LLaMA3B/train/zero-shot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365ee21-a7bd-454b-bec8-2f79b4e01bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ed69ff-9e25-488e-8681-adb76b425757",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ../datasets/gsm8k/LLaMA3B/test/zero-shot/ not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/LLaMA3B/test/zero-shot/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/datasets/load.py:2207\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2205\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2209\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2210\u001b[0m ):\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ../datasets/gsm8k/LLaMA3B/test/zero-shot/ not found"
     ]
    }
   ],
   "source": [
    "data= load_from_disk(f\"{data_path}/LLaMA3B/test/zero-shot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a8af750-5600-453e-bee0-464d3a22e4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7628bd-05f8-4a58-a623-286936d92a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_prompts=[]\n",
    "# for i in tqdm(range(len(data)), desc='preparing.....'):\n",
    "#     tokenized_prompt = TokensPrompt(prompt_token_ids=data['input_ids'][i])\n",
    "#     tokenized_prompts.append(tokenized_prompt)\n",
    "# # tokenized_prompts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68afe37c-2790-4816-97dc-67d2e0947dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-24 17:04:15 arg_utils.py:862] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-24 17:04:15 config.py:999] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 02-24 17:04:15 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 02-24 17:04:16 model_runner.py:915] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 02-24 17:04:16 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.46it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.25it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:04:18 model_runner.py:926] Loading model weights took 6.0160 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 17:04:18 gpu_executor.py:122] # GPU blocks: 19082, # CPU blocks: 2340\n",
      "INFO 02-24 17:04:21 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-24 17:04:21 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-24 17:04:30 model_runner.py:1335] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "# stop_strings =  ['<|eot_id|>', '<|start_header_id|>user<|end_header_id|>', 'Q:', '</s>', '<|im_end|>']\n",
    "\n",
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 # stop=stop_strings,\n",
    "                                 seed=1)\n",
    "\n",
    "model = LLM(\n",
    "        model=model_name, \n",
    "        # tokenizer=model_name, \n",
    "        tensor_parallel_size=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e7a5ce-ab58-4f06-832b-1d70f8b13231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1319/1319 [00:39<00:00, 33.55it/s, est. speed input: 4519.24 toks/s, output: 6232.58 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(data['input_ids'], sampling_params)\n",
    "# outputs = model.generate(['what is the capital of India','Where is Delhi'], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed0f9984-f4f5-4e5b-aafb-89d73c853a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs=[]\n",
    "for out in outputs:\n",
    "    generated_outputs.append({\n",
    "            \"input\": tokenizer.decode(out.prompt_token_ids, skip_special_tokens=False), \n",
    "            \"output\": [\n",
    "                ith_output.text for ith_output in out.outputs\n",
    "            ]    \n",
    "        })\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f46611d-3fe8-4429-b800-74d583bfb71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 Feb 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGiven the following problem, reason and give a final answer to the problem.\\nProblem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers\\' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers\\' market?\\nYour response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       "  'output': [\"To find out how much Janet makes every day at the farmers' market, we need to first calculate the total number of eggs she lays and then subtract the eggs she eats and bakes.\\n\\nJanet's ducks lay 16 eggs per day. She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left.\\n\\nShe bakes muffins with 4 eggs, so she has 13 - 4 = 9 eggs left.\\n\\nShe sells the remaining 9 eggs at the farmers' market for $2 per egg. To find the total amount she makes, we multiply the number of eggs she sells by the price per egg:\\n\\n9 eggs * $2/egg = $18\\n\\nSo, Janet makes $18 every day at the farmers' market.\\n\\nThe final answer is $18.\"]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392facd3-8670-4183-b35a-3bd6046cfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c50faaf8-251c-49c5-84ac-d51b3972cf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 41681]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp=tokenizer('prompt', add_special_tokens=True)['input_ids']\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c167135-0402-4ff9-990b-a11d470890cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>prompt'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inp, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f679309-b3f9-4cb2-b5af-6df66164119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Janet has 16 eggs per day. She eats 3 for breakfast, so she has 16 - 3 = 13 eggs left. She bakes muffins for 4, so she has 13 - 4 = 9 eggs left. She sells 9 eggs at $2 each, so she makes 9 x 2 = 18 dollars per day. The final answer is 18\n"
     ]
    }
   ],
   "source": [
    "for ex_outputs in all_outputs:\n",
    "    print(ex_outputs.prompt)\n",
    "    print(ex_outputs.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82969e11-6c15-4a32-9569-f448af340bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-24 16:02:37,552\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 16:02:38 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 02-24 16:02:39 model_runner.py:915] Starting to load model facebook/opt-125m...\n",
      "INFO 02-24 16:02:39 weight_utils.py:236] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.05it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.01it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 16:02:40 model_runner.py:926] Loading model weights took 0.2389 GB\n",
      "INFO 02-24 16:02:40 gpu_executor.py:122] # GPU blocks: 71292, # CPU blocks: 7281\n",
      "INFO 02-24 16:02:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-24 16:02:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-24 16:02:49 model_runner.py:1335] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 21.75it/s, est. speed input: 268.33 toks/s, output: 1087.78 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Translate the following English text to French: 'Hello, how are you?'\n",
      "Generated text: \n",
      "\n",
      "It is important to note that we speak French with two genders: on the one hand you have the masculine (Latin for \"fellow\", and the feminine \"other\") and on the other you have the feminine \"speaker\". Therefore,\n",
      "---\n",
      "Input: What is the capital of France?\n",
      "Generated text: \n",
      "The capital of France. It's the second largest country after Belgium and is the second biggest economy in the world. It's also the third largest nation in the world and it's the third largest country in the world.\n",
      "> it's the\n",
      "---\n",
      "Input: Explain the concept of machine learning in simple terms.\n",
      "Generated text: \n",
      "\n",
      "In the past, a scientific exploration of machine learning used to be regarded as an antiquated notion. It was not the first time that a researcher had to explain how an algorithm could be used to generate an algorithm.\n",
      "\n",
      "However, in\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.inputs import TokensPrompt\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "# Define your input strings\n",
    "input_strings = [\n",
    "    \"Translate the following English text to French: 'Hello, how are you?'\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the concept of machine learning in simple terms.\"\n",
    "]\n",
    "\n",
    "# Tokenize the input strings and create TokensPrompt instances\n",
    "tokenized_prompts = [\n",
    "    TokensPrompt(prompt_token_ids=tokenizer.encode(input_string))\n",
    "    for input_string in input_strings\n",
    "]\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=50)\n",
    "\n",
    "# Generate the outputs for the batch\n",
    "outputs = llm.generate(tokenized_prompts, sampling_params)\n",
    "\n",
    "# Print the results\n",
    "for input_string, output in zip(input_strings, outputs):\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Input: {input_string}\")\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a146a5b3-0380-43e4-a830-f5f8a82c03b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt_token_ids': [2,\n",
       "   19163,\n",
       "   19593,\n",
       "   5,\n",
       "   511,\n",
       "   2370,\n",
       "   2788,\n",
       "   7,\n",
       "   1515,\n",
       "   35,\n",
       "   128,\n",
       "   31414,\n",
       "   6,\n",
       "   141,\n",
       "   32,\n",
       "   47,\n",
       "   6600]},\n",
       " {'prompt_token_ids': [2, 2264, 16, 5, 812, 9, 1470, 116]},\n",
       " {'prompt_token_ids': [2,\n",
       "   43043,\n",
       "   1851,\n",
       "   5,\n",
       "   4286,\n",
       "   9,\n",
       "   3563,\n",
       "   2239,\n",
       "   11,\n",
       "   2007,\n",
       "   1110,\n",
       "   4]}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ee89d2-a273-4c92-8a7e-bbcb8b65a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def natural_sort_key(checkpoint_path):\n",
    "    \"\"\"Extracts numerical part from checkpoint names for correct sorting.\"\"\"\n",
    "    match = re.search(r\"checkpoint-(\\d+)\", checkpoint_path)\n",
    "    return int(match.group(1)) if match else float('inf')  # Send non-matching to the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d928475-51df-4912-8cff-9715191f2a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/exp-1.11/checkpoints/checkpoint-85\n",
      "../outputs/exp-1.11/checkpoints/checkpoint-170\n",
      "../outputs/exp-1.11/checkpoints/checkpoint-255\n",
      "../outputs/exp-1.11/checkpoints/checkpoint-340\n",
      "../outputs/exp-1.11/checkpoints/checkpoint-425\n"
     ]
    }
   ],
   "source": [
    "model_path = '../outputs/exp-1.11/checkpoints/'\n",
    "\n",
    "# Iterate over all checkpoints inside model_path\n",
    "# checkpoints = sorted([os.path.join(model_path, d) for d in os.listdir(model_path) if os.path.isdir(os.path.join(model_path, d))])\n",
    "checkpoints = [os.path.join(model_path, d) for d in os.listdir(model_path) if os.path.isdir(os.path.join(model_path, d))]\n",
    "\n",
    "# Sort numerically by extracting the number in \"checkpoint-XXXX\"\n",
    "checkpoints = sorted(checkpoints, key=natural_sort_key)\n",
    "\n",
    "if not checkpoints:\n",
    "    print(f\"No checkpoints found in {model_path}\")\n",
    "    \n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    checkpoint_name = os.path.basename(checkpoint) \n",
    "    print(os.path.join(model_path, checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499953a7-b177-46d2-89f7-7e51a2f666c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1372\n",
      "checkpoint-1029\n",
      "checkpoint-1710\n",
      "checkpoint-686\n",
      "checkpoint-343\n"
     ]
    }
   ],
   "source": [
    "for d in os.listdir(model_path):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6439920-f38d-45e5-bb53-13bf3bb1c38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
