{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a94d53-67a4-472a-805a-bb794d1f8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME']=cache_dir\n",
    "os.environ['HF_HUB_CACHE']=cache_dir+'/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e98810f-66cc-4bdf-8d0c-25bcee1ca525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-03-17 15:06:25,738\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from vllm.inputs import TokensPrompt\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "# from small_llm_reasoning.evaluation.gsm8k import get_score, eight_shot_messages\n",
    "# from small_llm_reasoning.generation.vllm_generation import llama_forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5664f354-d52f-4760-b461-1a8ead0c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data_path= \"../datasets/gsm8k\"\n",
    "split='feedback'\n",
    "data = load_from_disk(f\"{data_path}/tokenized/LLaMA3B-Instruct/{split}/zero-shot/\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7af0a8d-7d5c-4547-9f00-8fecb8ade87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# # import os\n",
    "\n",
    "# print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "# print(\"TRANSFORMERS_CACHE:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "# print(\"Hugging Face Cache Directory:\", hf_hub_download.__defaults__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b8ec1c-1fd8-4c54-9126-48c2a920ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "\n",
    "# model_name= \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name= \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name= \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_name= \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, cache_dir=cache_dir)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68afe37c-2790-4816-97dc-67d2e0947dcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 15:06:42 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir='/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 03-17 15:06:43 model_runner.py:915] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 03-17 15:06:43 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignored error while writing commit hash to /scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/refs/main: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/refs/main'.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.84it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 15:06:44 model_runner.py:926] Loading model weights took 6.0160 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 15:06:45 gpu_executor.py:122] # GPU blocks: 19048, # CPU blocks: 2340\n",
      "INFO 03-17 15:06:47 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-17 15:06:47 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-17 15:06:58 model_runner.py:1335] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "kwargs={\n",
    "    'download_dir':cache_dir,\n",
    "    'max_model_len': 2000\n",
    "}\n",
    "\n",
    "model = LLM(\n",
    "        model=model_name,  \n",
    "        tensor_parallel_size=1,\n",
    "        trust_remote_code=True,\n",
    "    **kwargs\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af3d5da6-71a5-4648-adff-fa593703f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_strings =  ['<|eot_id|>', '<|start_header_id|>user<|end_header_id|>', 'Q:', '</s>', '<|im_end|>']\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 stop=stop_strings,\n",
    "                                 logprobs=2\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67bea271-feb8-463f-919c-8f00ca6e7aec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in tqdm(range(1000)):\n",
    "#     assert len(data['input_ids'][i]['prompt_token_ids'])>0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4e7a5ce-ab58-4f06-832b-1d70f8b13231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it, est. speed input: 82.48 toks/s, output: 95.72 toks/s]\n"
     ]
    }
   ],
   "source": [
    "all_outputs = model.generate(data['input_ids'][143], sampling_params)\n",
    "# outputs = model.generate(['what is the capital of India','Where is Delhi'], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39ffdb0f-f59b-47ad-b00f-fedfa34f0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_strings =  ['<|eot_id|>', '<|start_header_id|>user<|end_header_id|>', 'Q:', '</s>', '<|im_end|>']\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 stop=stop_strings,\n",
    "                                 logprobs=2\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9f9dea5-0417-44f5-b1fa-d56b6da39500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [00:33<00:00, 30.21it/s, est. speed input: 3979.79 toks/s, output: 5209.60 toks/s]\n"
     ]
    }
   ],
   "source": [
    "all_outputs = model.generate(data['input_ids'], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72a7f341-541b-4d52-90df-63ab2055390e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generated_outputs=[]\n",
    "# for out in all_outputs:\n",
    "#     for o in out.outputs:\n",
    "#         # print(o.text)\n",
    "#         # print(len(o.token_ids))\n",
    "#         # print(o.token_ids)\n",
    "#         # print(type(o.logprobs))\n",
    "#         # print(len(o.logprobs))\n",
    "#         idx=0\n",
    "#         for token_log_probs in o.logprobs:\n",
    "#             assert len(token_log_probs.keys())==2\n",
    "#             assert list(token_log_probs.keys())[0]==o.token_ids[idx]\n",
    "#             print(o.token_ids[idx])\n",
    "#             print(token_log_probs)\n",
    "#             generated_outputs.append(o.token_ids[idx])\n",
    "#             idx+=1\n",
    "#             # print('\\n\\n')\n",
    "#             # print(len(token_log_probs.keys()))\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e26617b5-58c5-4f7a-bd21-583b9b98beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs=[]\n",
    "for out in all_outputs:\n",
    "    output={}\n",
    "    output['input']= tokenizer.decode(out.prompt_token_ids, skip_special_tokens=False)\n",
    "    output['output']=[]\n",
    "    output['token_ids']=[]\n",
    "    output['log_probs']=[]\n",
    "    output['all_returned_log_probs']=[]\n",
    "    for ith_output in out.outputs:\n",
    "        idx=0\n",
    "        output['output'].append(ith_output.text)\n",
    "        output['token_ids'].append(list(ith_output.token_ids))\n",
    "        log_probs=[]\n",
    "        for token_log_probs in ith_output.logprobs:\n",
    "            log_probs.append(token_log_probs[ith_output.token_ids[idx]].logprob)\n",
    "            returned_log_probs=[]\n",
    "            for token, logprob in token_log_probs.items():\n",
    "                returned_log_probs.append(\n",
    "                    {\n",
    "                        'token_id':token,\n",
    "                        'logprob':logprob.logprob,\n",
    "                        'rank':logprob.rank,\n",
    "                        'decoded_token':logprob.decoded_token\n",
    "                    }\n",
    "                )\n",
    "            output['all_returned_log_probs'].append(returned_log_probs)\n",
    "            idx+=1\n",
    "        output['log_probs'].append(log_probs)\n",
    "    generated_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ace7339-93a8-47a9-a909-420b836aabbe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(generated_outputs)):\n",
    "    try:\n",
    "        assert len(generated_outputs[i]['log_probs'][0])==len(generated_outputs[i]['all_returned_log_probs'])\n",
    "        assert len(generated_outputs[i]['log_probs'][0])==len(generated_outputs[i]['token_ids'][0])\n",
    "    except:\n",
    "        print(f'Index: {i}')\n",
    "        print(f'log_probs: {len(generated_outputs[i]['log_probs'][0])}')\n",
    "        print(f'all_returned_log_probs: {len(generated_outputs[i]['all_returned_log_probs'])}')\n",
    "        print(f'token_ids: {len(generated_outputs[i]['token_ids'][0])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfdd5f6b-3712-47bc-9842-a082baaa54ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_outputs[0]['log_probs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44946e1f-0740-4ff9-b870-8e2f46388151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_outputs[0]['all_returned_log_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f28af59-35a7-4247-9782-40abd4f7d29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_outputs[0]['token_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99b39bd-aa87-4330-9cc9-64d28be07bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs=True\n",
    "generated_outputs = [\n",
    "        {\n",
    "            'input': tokenizer.decode(out.prompt_token_ids, skip_special_tokens=False),\n",
    "            'output': [ith_output.text for ith_output in out.outputs],\n",
    "            **({\n",
    "                'log_probs': [\n",
    "                    [token_log_probs[token_id].logprob for token_log_probs, token_id \n",
    "                    in zip(ith_output.logprobs, ith_output.token_ids)]\n",
    "                    for ith_output in out.outputs\n",
    "                ]\n",
    "            } if log_probs else {})\n",
    "        }\n",
    "        for out in all_outputs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "291065e5-c7ca-4efa-9dc5-02d3568dd20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9710a6-058b-4d94-bd50-5db25fdf0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(generated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cf466be-2047-4c30-be29-fa06e6396400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>log_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To find out how many building blocks can fit ...</td>\n",
       "      <td>[[-4.458328112377785e-05, -0.13078656792640686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To find out how much Rachel should budget, we...</td>\n",
       "      <td>[[-0.0030838334932923317, -0.5552582740783691,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To find out how many kilograms of meat are le...</td>\n",
       "      <td>[[-0.016634341329336166, -0.3490457832813263, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To solve this problem, we need to find out ho...</td>\n",
       "      <td>[[-0.0503850020468235, -0.5876595973968506, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To find the total deposit amount, we need to ...</td>\n",
       "      <td>[[-0.026005588471889496, -0.5923835635185242, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To find the total number of crates of eggs Ga...</td>\n",
       "      <td>[[-0.05552699416875839, -0.5832895040512085, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To find the total number of passengers on the...</td>\n",
       "      <td>[[-0.03839171305298805, -0.40530502796173096, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To find the number of counters and marbles Re...</td>\n",
       "      <td>[[-0.02048187144100666, -0.6049375534057617, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To solve this problem, we need to find out ho...</td>\n",
       "      <td>[[-0.010952593758702278, -0.9995432496070862, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>[To solve this problem, let's break it down we...</td>\n",
       "      <td>[[-0.0795292779803276, -0.45551395416259766, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input  \\\n",
       "0    <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1    <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "2    <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "3    <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "4    <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "..                                                 ...   \n",
       "995  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "996  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "997  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "998  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "999  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "\n",
       "                                                output  \\\n",
       "0    [To find out how many building blocks can fit ...   \n",
       "1    [To find out how much Rachel should budget, we...   \n",
       "2    [To find out how many kilograms of meat are le...   \n",
       "3    [To solve this problem, we need to find out ho...   \n",
       "4    [To find the total deposit amount, we need to ...   \n",
       "..                                                 ...   \n",
       "995  [To find the total number of crates of eggs Ga...   \n",
       "996  [To find the total number of passengers on the...   \n",
       "997  [To find the number of counters and marbles Re...   \n",
       "998  [To solve this problem, we need to find out ho...   \n",
       "999  [To solve this problem, let's break it down we...   \n",
       "\n",
       "                                             log_probs  \n",
       "0    [[-4.458328112377785e-05, -0.13078656792640686...  \n",
       "1    [[-0.0030838334932923317, -0.5552582740783691,...  \n",
       "2    [[-0.016634341329336166, -0.3490457832813263, ...  \n",
       "3    [[-0.0503850020468235, -0.5876595973968506, -0...  \n",
       "4    [[-0.026005588471889496, -0.5923835635185242, ...  \n",
       "..                                                 ...  \n",
       "995  [[-0.05552699416875839, -0.5832895040512085, -...  \n",
       "996  [[-0.03839171305298805, -0.40530502796173096, ...  \n",
       "997  [[-0.02048187144100666, -0.6049375534057617, -...  \n",
       "998  [[-0.010952593758702278, -0.9995432496070862, ...  \n",
       "999  [[-0.0795292779803276, -0.45551395416259766, -...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6439920-f38d-45e5-bb53-13bf3bb1c38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[0]['log_probs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c4c4596-13ab-4ffb-a65b-a78fd77654f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find out how many building blocks can fit into the box, we need to divide the volume of the box by the volume of a single building block.\\n\\nThe volume of the box is calculated by multiplying its height, width, and length:\\nVolume of box = height * width * length = 8 * 10 * 12 = 960 cubic inches\\n\\nThe volume of a single building block is calculated by multiplying its height, width, and length:\\nVolume of building block = height * width * length = 3 * 2 * 4 = 24 cubic inches\\n\\nNow, we divide the volume of the box by the volume of a single building block to find out how many blocks can fit:\\nNumber of blocks = Volume of box / Volume of building block = 960 / 24 = 40\\n\\nThe final answer is 40.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e8175-fdd8-457c-901a-75e8edfd7ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
