{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a94d53-67a4-472a-805a-bb794d1f8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME']=cache_dir\n",
    "os.environ['HF_HUB_CACHE']=cache_dir+'/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e98810f-66cc-4bdf-8d0c-25bcee1ca525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from vllm.inputs import TokensPrompt\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5664f354-d52f-4760-b461-1a8ead0c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'userid', 'cefr', 'text', 'edits'],\n",
       "    num_rows: 350\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data_path= \"../datasets/gec/\"\n",
    "split='val'\n",
    "data=load_from_disk(data_path+split)\n",
    "# data = load_from_disk(f\"{data_path}/tokenized/LLaMA8B-Instruct/{split}/0-shot/\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bc4cba-4fca-483e-8b66-c811d7ea4f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1-352420',\n",
       " 'userid': '21658',\n",
       " 'cefr': 'A2.ii',\n",
       " 'text': 'It\\'s difficult answer at the question \"what are you going to do in the future?\" if the only one who has to know it is in two minds. When I was younger I used to say that I wanted to be a teacher, a saleswoman and even a butcher.. I don\\'t know why. I would like to study Psychology because one day I would open my own psychology office and help people. It\\'s difficult because I\\'ll have to study hard and a lot, but I think that if you like a subject, you\\'ll study it easier. Maybe I\\'ll change my mind, maybe not.',\n",
       " 'edits': {'start': [14, 21, 150, 228, 280, 304, 328, 466],\n",
       "  'end': [14, 24, 150, 229, 280, 304, 334, 472],\n",
       "  'text': [' to', '', ',', '', ',', ' like to', 'clinic', 'more easily']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b8ec1c-1fd8-4c54-9126-48c2a920ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "\n",
    "model_name= \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68afe37c-2790-4816-97dc-67d2e0947dcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-1B-Instruct/.no_exist/9213176726f574b556790deb65791e0c5aa438b6/preprocessor_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-30 19:57:14 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir='/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 04-30 19:57:16 model_runner.py:915] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 04-30 19:57:16 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignored error while writing commit hash to /scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-1B-Instruct/refs/main: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-1B-Instruct/refs/main'.\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-1B-Instruct/.no_exist/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors.index.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-30 19:57:17 weight_utils.py:280] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6120ba2c284d00af84cc966280a2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-30 19:57:22 model_runner.py:926] Loading model weights took 2.3185 GB\n",
      "INFO 04-30 19:57:22 gpu_executor.py:122] # GPU blocks: 73993, # CPU blocks: 8192\n",
      "INFO 04-30 19:57:24 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-30 19:57:24 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-30 19:57:35 model_runner.py:1335] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "kwargs={\n",
    "    'download_dir':cache_dir,\n",
    "    'max_model_len': 2000\n",
    "}\n",
    "\n",
    "model = LLM(\n",
    "        model=model_name,  \n",
    "        tensor_parallel_size=1,\n",
    "        trust_remote_code=True,\n",
    "    **kwargs\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af3d5da6-71a5-4648-adff-fa593703f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_strings =  ['<|eot_id|>', '<|start_header_id|>user<|end_header_id|>', 'Q:', '</s>', '<|im_end|>']\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 stop=stop_strings,\n",
    "                                 # logprobs=2\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57406379-b68a-4792-b4c7-68672d73c124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_prompt': 'Given an input, the goal is to detect and correct the grammatical errors of all error types present in the sentence and return the reasoning first and then the corrected sentence.\\n\\nYour response should end with \"The corrected text is [answer]\" where [answer] is the grammatically correct version of the input sentence.',\n",
       " 'system_msg': 'You are an expert in removing subjective biases in texts.',\n",
       " 'user_msg': 'Instruction:\\n{instruction}\\n\\nInput:\\n{question}\\n\\n',\n",
       " 'assistant_msg': '{rationale} The corrected text is {response}\\n'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "task='gec'\n",
    "task_prompt_path=f'../prompts/{task}.json'\n",
    "with open(task_prompt_path) as fp:\n",
    "    task_prompt = json.load(fp)\n",
    "task_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd2e85fb-e2e7-4a58-b434-ada722ce989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 30 Apr 2025\n",
      "\n",
      "You are an expert in removing subjective biases in texts.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Instruction:\n",
      "Given an input, the goal is to detect and correct the grammatical errors of all error types present in the sentence and return the reasoning first and then the corrected sentence.\n",
      "\n",
      "Your response should end with \"The corrected text is [answer]\" where [answer] is the grammatically correct version of the input sentence.\n",
      "\n",
      "Input:\n",
      "Dear Jo,\n",
      "\n",
      "How are you? I'm writing to inform you that some advice on travelling and working in my country.\n",
      "\n",
      "On the one hand, in my country there are a lot of place to travel. For example Barcelona,Madrid,Sevilla,Valencia. There are wonderful cities because there are a lot of things to visit for example in Madrid you can visit \"Puerta del Sol\", \"Las Cibeles\". I visit Madrid twice a year. All summer I visit Sevilla because I love this city and I suggest that you visit Sevilla.\n",
      "\n",
      "On the other hand,in my country there are a few job because the economy is very bad. But, if you want I look for a job from you. My best friend looking for a people responsible for look after her sister. Her sister is 6 years old and you should look after every weekend. You will be interesting in this job?\n",
      "\n",
      "Thank you for you e-mail, it was wonderful to hear from you.\n",
      "\n",
      "I'm looking forward to seeing you.\n",
      "\n",
      "Take care, Laura.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat=[\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":task_prompt['system_msg'],\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":task_prompt['user_msg'].format(instruction=task_prompt['task_prompt'], question=data[5]['text'])\n",
    "    }\n",
    "]\n",
    "prompt=tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4e7a5ce-ab58-4f06-832b-1d70f8b13231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it, est. speed input: 107.77 toks/s, output: 167.87 toks/s]\n"
     ]
    }
   ],
   "source": [
    "all_outputs = model.generate(prompt, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f803a8c1-e8da-4165-b8e4-88509c05993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sentence contains the following grammatical errors:\n",
      "\n",
      "1. \"inform you\" should be \"inform you\" (subject-verb agreement)\n",
      "2. \"some advice on travelling\" should be \"some advice on traveling\" (subject-verb agreement)\n",
      "3. \"On the one hand\" should be \"On the one hand\" (capitalization)\n",
      "4. \"there are a lot of place to travel\" should be \"there are a lot of places to travel\" (subject-verb agreement)\n",
      "5. \"For example\" should be \"For example\" (capitalization)\n",
      "6. \"Puerta del Sol\" and \"Las Cibeles\" should be \"Puerta del Sol\" and \"Las Cibeles\" (capitalization)\n",
      "7. \"I visit Madrid twice a year\" should be \"I visit Madrid twice a year\" (subject-verb agreement)\n",
      "8. \"I love this city\" should be \"I love this city\" (subject-verb agreement)\n",
      "9. \"I suggest that you visit Sevilla\" should be \"I suggest that you visit Sevilla\" (subject-verb agreement)\n",
      "10. \"My best friend looking for a people responsible for look after her sister\" should be \"My best friend looking for a person responsible for looking after her sister\" (subject-verb agreement)\n",
      "11. \"Her sister is 6 years old\" should be \"Her sister is 6 years old\" (subject-verb agreement)\n",
      "12. \"You should look after every weekend\" should be \"You should look after every weekend\" (subject-verb agreement)\n",
      "13. \"You will be interesting in this job\" should be \"You will be interesting in this job\" (subject-verb agreement)\n",
      "14. \"Thank you for you e-mail\" should be \"Thank you for your email\" (subject-verb agreement)\n",
      "15. \"Take care\" should be \"Take care\" (subject-verb agreement)\n",
      "16. \"Laura\" should be \"Laura\" (capitalization)\n",
      "17. \"I'm looking forward to seeing you\" should be \"I'm looking forward to seeing you\" (subject-verb agreement)\n",
      "18. \"Take care, Laura\" should be \"Take care, Laura\" (capitalization)\n",
      "\n",
      "The corrected text is:\n",
      "\n",
      "Dear Jo,\n",
      "\n",
      "How are you? I'm writing to inform you that some advice on traveling and working in my country.\n",
      "\n",
      "On the one hand, in my country there are a lot of places to travel. For example,\n"
     ]
    }
   ],
   "source": [
    "print(all_outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a61a2a2-ebf1-40fd-a2ec-47053f4d13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': [14, 21, 150, 228, 280, 304, 328, 466],\n",
       " 'end': [14, 24, 150, 229, 280, 304, 334, 472],\n",
       " 'text': [' to', '', ',', '', ',', ' like to', 'clinic', 'more easily']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['edits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e9bad-5090-42f7-9c98-5ffa41430834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
