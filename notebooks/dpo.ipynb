{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2950fab0-b612-42f5-bd72-368d64a2c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME']=cache_dir\n",
    "os.environ['HF_HUB_CACHE']=cache_dir+'/hub'\n",
    "hf_token=os.getenv('hf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f801af0f-6703-440c-bf59-ee0515ee9cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_from_disk, Dataset, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b1752-a43b-4add-bfba-6ab98a6f11a8",
   "metadata": {},
   "source": [
    "### Preparing data in preference format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "492ff1fd-dd40-41ce-9e42-fe13a8c18948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected', 'logprob_ratio', 'tr_prob', 'tr_answer', 'stu_answer', 'tr_score'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_path='../datasets/gsm8k/dpo/dpo_data_tr_prob_threshold'\n",
    "data_path='../datasets/neutralization/dpo/LLaMA3B/feedback-100/dpo_data_with_teacher_gen'\n",
    "dpo_data=load_from_disk(data_path)\n",
    "dpo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c89d5a35-6b3d-489a-89a5-2390fd03cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an expert in removing subjective biases in texts .<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Instruction:\n",
      "Given an input sentence, reason and replace the word with subjective bias to a word with neutral point of view. Consider the following types of biases:\n",
      "1. framing biases use subjective words linked with a particular point of view (e.g. using words like best or deepest or using pilfered from instead of based on);\n",
      "2. epistemological biases are linguistic features that subtly (often via presupposition) modify the believability of a proposition;\n",
      "3. demographic biases are texts with presuppositions about particular genders, races, or other demographic categories (e.g. presupposing that all programmers are male).\n",
      "\n",
      "Your response should end with \"The neutralized text is [answer]\" where [answer] is the neutralized version of the input sentence.\n",
      "\n",
      "Input:\n",
      "flashpacking refers to an affluent backpacker.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      " The neutralized text is  flashpacking is a neologism used to refer to an affluent backpacker.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dpo_data['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e8176ef-aeb9-4fdc-8817-a17d96582f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To neutralize the sentence, we need to address potential biases. The term \"affluent\" might be seen as having a slightly positive connotation, implying a certain level of respect or admiration for the individual's financial status. However, the primary subjective element here is the term \"affluent\" itself, which can be perceived as having a positive bias due to its association with wealth.\n",
      "\n",
      "A more neutral approach would be to simply state the fact without implying a value judgment on the financial status. Thus, instead of \"affluent,\" we could use a term that simply describes the situation without the positive connotation, such as \"well-funded\" or, more simply, describe the behavior or characteristics without focusing on the financial aspect.\n",
      "\n",
      "The neutralized text is: flashpacking refers to a backpacker with a high budget.\n"
     ]
    }
   ],
   "source": [
    "print(dpo_data['chosen'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7ad0e52-869e-4d6c-9a56-b6e61b0f6878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To neutralize the subjective bias in the given sentence, we need to replace the word \"flashpacking\" with a more neutral term.\n",
      "\n",
      "\"Flashpacking\" is a term that is often associated with a particular point of view, implying that it is a luxurious or indulgent form of backpacking. To replace it with a neutral term, we can use the word \"backpacking\".\n",
      "\n",
      "The neutralized text is Backpacking refers to an affluent traveler.\n"
     ]
    }
   ],
   "source": [
    "print(dpo_data['rejected'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f96c1fe-6c0d-4a0e-b3af-e5af52f39345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def formatting_prompts_func_gec(example):\n",
    "    with open('../prompts/gec.json') as fp:\n",
    "        task_prompt = json.load(fp)\n",
    "    system_msg= f'<|start_header_id|>system<|end_header_id|>\\n\\n{task_prompt['system_msg']}<|eot_id|>'\n",
    "    user_msg= f'<|start_header_id|>user<|end_header_id|>\\n\\n{task_prompt['user_msg'].format(instruction=task_prompt['task_prompt'], question=example['input'])}<|eot_id|>'\n",
    "    rationale= example['rationale'] if 'rationale' in example else '' # vanilla sft using off-the-shelf data doesn't have rationale\n",
    "    # For GEC multiple editors are allowed, so when we use off-the-shelf data we need to choose one of the output or else train it as n different examples\n",
    "    # Using to determine off-the-shelf data or custom data\n",
    "    if 'rationale' in example:\n",
    "        assistant_msg=f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{task_prompt['assistant_msg'].format(response=example['output'], rationale=rationale)}<|eot_id|>' \n",
    "    else: # Currently taking first output for off-the-shelf data\n",
    "        assistant_msg=f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{task_prompt['assistant_msg'].format(response=example['output'][0], rationale=rationale)}<|eot_id|>' \n",
    "    text= system_msg + user_msg + assistant_msg\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2edede97-c7ea-4c5a-bca9-5fde81c15938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9834',\n",
       " 'input': 'However , it has a nice and three bathrooms .',\n",
       " 'edits': ['A 0 1|||R:OTHER|||In addition|||REQUIRED|||-NONE-|||0',\n",
       "  'A 5 6|||UNK|||nice|||REQUIRED|||-NONE-|||0'],\n",
       " 'output': ['In addition , it has a nice and three bathrooms .']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=load_from_disk('../datasets/gec/raw/feedback-100')\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448800b-61d4-4243-919f-4aac22dca140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf630d39-1812-4fac-a9d9-51fe310be4b7",
   "metadata": {},
   "source": [
    "## DPO Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9d3b47-112c-456e-ba53-ed94b578be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dpo.py\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "# from trl import DataCollatorForPreference\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from small_llm_reasoning.trainer.dpo_trainer import CustomDPOTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b9fd75-884e-40b8-94e4-906b7116d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='meta-llama/Llama-3.2-3B-Instruct'\n",
    "output_dir= 'dpo_data_tr_stu_correctness_2'\n",
    "add_special_tokens= True\n",
    "epochs= 5\n",
    "lr=1e-5 \n",
    "lr_scheduler_type= 'cosine'\n",
    "warmup= 0.1 \n",
    "weight_decay= 0.01\n",
    "per_device_train_batch_size= 4\n",
    "gradient_accumulation_steps= 4\n",
    "max_length= 500 \n",
    "torch_dtype='bfloat16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802f7208-6860-4630-a01d-059535dafc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/.no_exist/0cb88a4f764b7a12671c53f0838cd831a0843b95/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/.no_exist/0cb88a4f764b7a12671c53f0838cd831a0843b95/adapter_config.json'\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token, \n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.add_special_tokens=add_special_tokens\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch_dtype,\n",
    "    token=hf_token, \n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a93205c-4769-40a4-a597-279ca5b81ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math.ceil((dpo_data.num_rows*epochs)/(per_device_train_batch_size*gradient_accumulation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d3c339-450b-4aa8-ae21-3e12554d425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = DPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=lr, # Default 1e-6\n",
    "    lr_scheduler_type=lr_scheduler_type,        \n",
    "    weight_decay=weight_decay,\n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_ratio=warmup,\n",
    "    logging_steps=10,\n",
    "    dataloader_drop_last=False,\n",
    "    # max_steps=math.ceil((dpo_data.num_rows*epochs)/(per_device_train_batch_size*gradient_accumulation_steps)),\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07c43d8-64fb-4b4b-aae3-d8dccd920ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize trainer\n",
    "# trainer = DPOTrainer(\n",
    "#     model=model, \n",
    "#     args=training_args, \n",
    "#     processing_class=tokenizer, \n",
    "#     train_dataset=dpo_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f70e7e4-745d-4e1e-b077-10911428b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomDPOTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    processing_class=tokenizer, \n",
    "    train_dataset=dpo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d81440a-59be-4f5a-b037-0c9e11d730bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_epochs: 5\n",
      "num_update_steps_per_epoch: 4\n",
      "num_examples: 71\n",
      "num_train_samples: 355\n",
      "epoch_based: True\n",
      "len_dataloader: 18\n",
      "max_steps: 20\n",
      "callback: [<transformers.trainer_callback.DefaultFlowCallback object at 0x71fff7809fa0>, <transformers.utils.notebook.NotebookProgressCallback object at 0x71fff7bdad80>]\n",
      "Total_updates: 5\n",
      "Update-Step: 0\n",
      "before on_step_end: [<transformers.trainer_callback.DefaultFlowCallback object at 0x71fff7809fa0>, <transformers.utils.notebook.NotebookProgressCallback object at 0x71fff7bdad80>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/20 : < :, Epoch 0.22/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after on_step end: [<transformers.trainer_callback.DefaultFlowCallback object at 0x71fff7809fa0>, <transformers.utils.notebook.NotebookProgressCallback object at 0x71fff7bdad80>]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kedar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/Small-LLM-Reasoning/src/small_llm_reasoning/trainer/dpo_trainer.py:397\u001b[0m, in \u001b[0;36mCustomDPOTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter on_step end: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 397\u001b[0m     \u001b[43mkedar\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# self._maybe_log_save_evaluate(\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;66;03m#     tr_loss,\u001b[39;00m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m#     grad_norm,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kedar' is not defined"
     ]
    }
   ],
   "source": [
    "output=trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d693e-7f0e-4d0d-bbb8-d4d3efaacb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_ratio(teacher_log_prob, student_log_prob):\n",
    "    tr_stu_logprob=[]\n",
    "    student_logprob=[]\n",
    "    teacher_logprob=[]\n",
    "    for i in range(len(teacher_log_prob)):\n",
    "        student_log_probs=np.array(student_log_prob[i])\n",
    "        teacher_log_probs=np.array(teacher_log_prob[i])\n",
    "        student_logprob.append(np.mean(student_log_probs))\n",
    "        teacher_logprob.append(np.mean(teacher_log_probs))\n",
    "        tr_stu_logprob.append(\n",
    "            np.subtract(\n",
    "                np.mean(teacher_log_probs),\n",
    "                np.mean(student_log_probs)\n",
    "            )\n",
    "        )\n",
    "                \n",
    "    return tr_stu_logprob\n",
    "def merge_and_sample_data(data,teacher_data,student_data, remove_incorrects, sampling_ratio, seed=42 ):\n",
    "    # print(teacher_data['log_probs'][1])\n",
    "    # print(len(teacher_data['log_probs']))\n",
    "    tr_stu_logprob_ratio=get_log_prob_ratio(teacher_data['log_probs'],student_data['student_log_probs'])\n",
    "    threshold=np.median(tr_stu_logprob_ratio)\n",
    "    teacher_answers=[]\n",
    "    teacher_scores=[]\n",
    "    print(f'Median teacher-student-logprob-ratio: {threshold}')\n",
    "    for i in range(teacher_data.num_rows):\n",
    "        teacher_answers.append(teacher_data['output'][i][0])\n",
    "        teacher_scores.append(teacher_data['score'][i])\n",
    "    questions=data['question']\n",
    "    new_data = {\n",
    "        'question': questions,\n",
    "        'answer': teacher_answers,\n",
    "        'score': teacher_scores,\n",
    "        'logprob_ratio':tr_stu_logprob_ratio\n",
    "    }\n",
    "    \n",
    "    data= Dataset.from_dict(new_data)\n",
    "\n",
    "    if remove_incorrects:\n",
    "        data= data.filter(lambda x: x['score']==1)\n",
    "    print(f'After removing incorrects from teacher:{data.num_rows}')\n",
    "    \n",
    "    \n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    total_size = len(data)\n",
    "    size_below = int(total_size * sampling_ratio)\n",
    "    size_above = total_size - size_below\n",
    "\n",
    "    below_thresh = data.filter(lambda example: example['logprob_ratio'] < threshold)\n",
    "    above_thresh = data.filter(lambda example: example['logprob_ratio'] >= threshold)\n",
    "    print(f'below threshold:{below_thresh.num_rows}')\n",
    "    print(f'above threshold:{above_thresh.num_rows}')\n",
    "\n",
    "    def upsample(ds, target_size):\n",
    "        if len(ds) == 0:\n",
    "            return ds  # Avoid divide-by-zero\n",
    "        indices = [rng.randint(0, len(ds) - 1) for _ in range(target_size)]\n",
    "        return ds.select(indices)\n",
    "\n",
    "    sampled_below = upsample(below_thresh, size_below)\n",
    "    sampled_above = upsample(above_thresh, size_above)\n",
    "\n",
    "    data = concatenate_datasets([sampled_below, sampled_above])\n",
    "    return data.shuffle(seed=seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d007284-e988-4c83-be0d-adb099e559ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, Dataset, load_dataset, concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c43fd5e9-c7c6-4b53-a1d9-72b6f1a9c209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected', 'tr_prob', 'tr_score', 'tr=stu'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path='../datasets/gsm8k/dpo/dpo_data_with_teacher_gen'\n",
    "data=load_from_disk(data_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7169a06c-449b-4c47-9d6b-a5d08a14a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_col='tr_prob'\n",
    "sampling_ratio=0.9\n",
    "seed=42\n",
    "threshold=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2069656-d8cb-48f0-a44c-d0b07820f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(seed)\n",
    "\n",
    "total_size = len(data)\n",
    "size_below = int(total_size * sampling_ratio)\n",
    "size_above = total_size - size_below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db410f7-82fe-4a11-9d0c-b1439686354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 52634.07 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 133926.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "below threshold:93\n",
      "above threshold:907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected', 'tr_prob', 'tr_answer', 'stu_answer', 'tr_score'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_thresh = data.filter(lambda example: example[sampling_col] < threshold)\n",
    "above_thresh = data.filter(lambda example: example[sampling_col] >= threshold)\n",
    "print(f'below threshold:{below_thresh.num_rows}')\n",
    "print(f'above threshold:{above_thresh.num_rows}')\n",
    "\n",
    "def upsample(ds, target_size):\n",
    "    if len(ds) == 0:\n",
    "        return ds  # Avoid divide-by-zero\n",
    "    indices = [rng.randint(0, len(ds) - 1) for _ in range(target_size)]\n",
    "    return ds.select(indices)\n",
    "\n",
    "sampled_below = upsample(below_thresh, size_below)\n",
    "sampled_above = upsample(above_thresh, size_above)\n",
    "\n",
    "data = concatenate_datasets([sampled_below, sampled_above])\n",
    "data=data.shuffle(seed=seed)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84118cd-f5e7-4748-a397-f14b37b489d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col=[]\n",
    "for i in range(data.num_rows):\n",
    "    new_col.append(data['tr_answer'][i]!=data['stu_answer'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca43a9fb-285d-422f-a9d0-3864bc530dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 13977.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected', 'tr_prob', 'tr_answer', 'stu_answer', 'tr_score', 'new_col'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda example: {\"tr!=stu\": example[\"tr_answer\"] != example[\"stu_answer\"]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22975391-b8c2-4fbb-b05a-da86165bbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_data_with_teacher_gen_by_sampling(data_path, sampling_col, remove_incorrects=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a05a35-4993-48b8-b98a-e5ea098f2c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ac35c-a141-4493-93bc-04b92a539208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebffb4b0-b5a1-44ac-91ef-1064ea47f453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6554ac-7e93-47f8-9ffa-8525a2583fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2ff627-ec43-48c1-9303-453820d82a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME']=cache_dir\n",
    "os.environ['HF_HUB_CACHE']=cache_dir+'/hub'\n",
    "hf_token=os.getenv('hf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07daf8f-b671-4ebe-b38b-c36380e05f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441d79e3-ee3b-4182-83bf-3c298b916ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='meta-llama/Llama-3.2-3B-Instruct'\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "chat=[\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content':'Hello World!'\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c420de-e4cf-4e73-934b-1930593c7403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHello World!<|eot_id|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(chat,tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92097bf8-da8f-4c9f-9ef8-4024e96ab9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
