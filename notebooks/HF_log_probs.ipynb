{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db724bf-8c7a-4b3c-8023-cc39182e7ff1",
   "metadata": {},
   "source": [
    "### STUDENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fcffc6-0fd2-4ab5-8c3b-6d01e729db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME']=cache_dir\n",
    "os.environ['HF_HUB_CACHE']=cache_dir+'/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f970ef-4903-48bf-95d1-f12dafac5b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e6f40f-2c36-4215-acc0-c22e14f48d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'token_ids', 'log_probs', 'all_returned_log_probs', 'model_answer', 'GT_Answer', 'score'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_student = load_dataset('json',data_files='../outputs/exp-2.0.1/eval_1/generated_outputs.json')['train']\n",
    "data_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e2d65d-9493-4301-8223-981bf292cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find out how many building blocks can fit into the box, we need to divide the volume of the box by the volume of a single building block.\\n\\nThe volume of the box is calculated by multiplying its height, width, and length:\\nVolume of box = height * width * length = 8 * 10 * 12 = 960 cubic inches\\n\\nThe volume of a single building block is calculated by multiplying its height, width, and length:\\nVolume of building block = height * width * length = 3 * 2 * 4 = 24 cubic inches\\n\\nNow, we divide the volume of the box by the volume of a single building block to find out how many blocks can fit:\\nNumber of blocks = Volume of box / Volume of building block = 960 / 24 = 40\\n\\nThe final answer is 40.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_student['output'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23502eb-ebc1-4d6f-9734-11b33de613a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data_path= \"../datasets/gsm8k/tokenized/LLaMA3B-Instruct/feedback/zero-shot/\"\n",
    "data_tokenized = load_from_disk(data_path)\n",
    "data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbf0dd9-792b-4716-80c0-4bb81e765be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../datasets/gsm8k/feedback/\"\n",
    "data_gt = load_from_disk(data_path)\n",
    "data_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5ae950-2130-4bcd-abaf-815dd306aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The volume of the box is 8 x 10 x 12 = <<8*10*12=960>>960 cu in.\\nThe volume of a wooden block is 3 x 2 x 4 = <<3*2*4=24>>24 cu in.\\n960/24 = <<960/24=40>>40 wooden blocks can fit into the box.\\n#### 40'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gt['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed04fcc-353e-4e51-b7e6-cefeaa38ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "padding='longest'\n",
    "padding_side=None\n",
    "special_tokens=False\n",
    "torch_dtype='bfloat16'\n",
    "hf_token=os.getenv('hf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26562e-b3a1-407a-9008-b8e44b183f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/.no_exist/0cb88a4f764b7a12671c53f0838cd831a0843b95/adapter_config.json'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token, \n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "device='cuda'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch_dtype,\n",
    "    token=hf_token, \n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model.eval()\n",
    "if padding_side:\n",
    "        tokenizer.padding_side = padding_side\n",
    "add_special_tokens = {\"add_special_tokens\": special_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98c40eb-89a1-41fa-983c-8c9093d30147",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "all_outputs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99caf247-fa0f-4547-a241-2823ab98a1e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 125/125 [03:45<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,data_student.num_rows,batch_size)):\n",
    "    \n",
    "    examples=[]\n",
    "    questions=[]\n",
    "    answers=[]\n",
    "    for j in range(i, min(i + batch_size, data_student.num_rows)):\n",
    "        question = torch.tensor(data_tokenized['input_ids'][j]['prompt_token_ids'], dtype=torch.long).unsqueeze(0)\n",
    "        answer = torch.tensor(data_student['token_ids'][j][0], dtype=torch.long).unsqueeze(0)\n",
    "        examples.append(torch.cat((question, answer), dim=1).squeeze(dim=0))\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        \n",
    "    # **Pad after concatenation**\n",
    "    examples =tokenizer.pad(\n",
    "        {\"input_ids\": examples},\n",
    "        padding=True,  # Pads to longest sequence in batch\n",
    "        return_tensors=\"pt\"  # Convert to PyTorch tensor\n",
    "    )['input_ids'].to(model.device)\n",
    "\n",
    "    # print(f'Example Shape:{examples.shape}')\n",
    "    \n",
    "    # Forward Pass\n",
    "    outputs = model(examples)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "    # print(f'Logits Shape: {outputs.logits.shape}')\n",
    "    probs = probs[:, :-1, :]\n",
    "    examples = examples[:, 1:]\n",
    "    \n",
    "    # print(f'Example Shape:{examples.shape}')\n",
    "    # print(f'Probs Shape: {probs.shape}')\n",
    "    \n",
    "    gen_probs = torch.gather(probs, 2, examples[:, :, None]).squeeze(-1)\n",
    "    # print(f'GenProbs Shape:{gen_probs.shape}')\n",
    "    \n",
    "    for j in range(examples.shape[0]):\n",
    "        # print(f'Question Shape:{questions[j].shape}')\n",
    "        answer_start_idx = questions[j].shape[1]-1\n",
    "        answer_end_idx = answer_start_idx + answers[j].shape[1]\n",
    "        logprobs=[]\n",
    "        for token, prob in zip(examples[j][answer_start_idx:answer_end_idx], gen_probs[j][answer_start_idx:answer_end_idx]):\n",
    "            logprobs.append(prob.item())\n",
    "            # print(f'{token}:\\t{prob.item()}\\t\\t{torch.exp(torch.tensor(prob.item()))}')\n",
    "        all_outputs.append(\n",
    "            {\n",
    "                'prompt':data_student['input'][i+j],\n",
    "                'gt_reasoning':data_gt['answer'][i+j],\n",
    "                'gt_answer':data_student['GT_Answer'][i+j],\n",
    "                'student_token_ids':data_student['token_ids'][i+j][0],\n",
    "                'student_reasoning':data_student['output'][i+j][0],\n",
    "                'student_answer':data_student['model_answer'][i+j][0],\n",
    "                'student_correctness':data_student['score'][i+j],\n",
    "                'student_log_probs':logprobs\n",
    "            }\n",
    "        )\n",
    "    # Clearing memory to avoid OOM issues\n",
    "    del examples, outputs, probs, gen_probs, logprobs, questions, answers\n",
    "    gc.collect()  # Trigger Python's garbage collector\n",
    "    torch.cuda.empty_cache()  # Free unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf77398f-2bb9-46e8-81ba-434b875e2460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb7ee41c-7241-4617-a174-7a22ccf52f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('logprobs.json', \"w\") as f:\n",
    "    json.dump(all_outputs, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432cf645-674f-4065-8718-8b58db70695a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beb5cde2-dd69-4169-b15f-2355a83ca864",
   "metadata": {},
   "source": [
    "### Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f00255d-8d9d-4de6-8b27-bf4d891c1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "# os.environ['HF_HOME']=cache_dir\n",
    "# os.environ['HF_HUB_CACHE']=cache_dir+'/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "136fc888-18f8-4f2c-ade1-b6bba23c3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import gc\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from datasets import load_from_disk, load_dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbf7aa08-7a73-4d62-be3d-c110300f69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_student = load_dataset('json',data_files='../outputs/exp-2.0.1/eval_1/generated_outputs.json')['train']\n",
    "# data_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "467af400-342f-4212-8c91-34d1071e6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading data\n",
    "# data_path= \"../datasets/gsm8k/tokenized/LLaMA3B-Instruct/feedback/zero-shot/\"\n",
    "# data_tokenized = load_from_disk(data_path)\n",
    "# data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47184565-a06a-41d4-abfe-dff8cc33709e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'token_ids', 'log_probs', 'all_returned_log_probs', 'model_answer', 'GT_Answer', 'score'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_teacher = load_dataset('json',data_files='../outputs/exp-2.0.3/eval_1/generated_outputs.json')['train']\n",
    "data_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa90974-0035-4dc4-b58b-32b3f428e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "padding='longest'\n",
    "padding_side=None\n",
    "special_tokens=False\n",
    "torch_dtype='bfloat16'\n",
    "hf_token=os.getenv('hf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200001ae-5755-4c8b-9acd-0f391612b3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.3-70B-Instruct/.no_exist/6f6073b423013f6a7d4d9f39144961bfbfbc386b/adapter_config.json'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.3-70B-Instruct/.no_exist/6f6073b423013f6a7d4d9f39144961bfbfbc386b/adapter_config.json'\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [00:28<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token, \n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch_dtype,\n",
    "    token=hf_token, \n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model.eval()\n",
    "if padding_side:\n",
    "        tokenizer.padding_side = padding_side\n",
    "add_special_tokens = {\"add_special_tokens\": special_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9104f7-a5e5-4560-ac81-5e61f084b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e18d2dc8-a71d-44c0-9104-348599ca3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Open and read the JSON file\n",
    "with open('logprobs.json', 'r') as file:\n",
    "    all_outputs = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c440ed9-1ca1-4fdd-b305-29de5e9c0986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2edd4fe5-03e0-4498-8470-c8c7bc4917e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 1000/1000 [09:57<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,data_student.num_rows,batch_size)):\n",
    "    examples=[]\n",
    "    questions=[]\n",
    "    answers=[]\n",
    "    for j in range(i, min(i + batch_size, data_student.num_rows)):\n",
    "        question = torch.tensor(data_tokenized['input_ids'][j]['prompt_token_ids'], dtype=torch.long).unsqueeze(0)\n",
    "        answer = torch.tensor(data_student['token_ids'][j][0], dtype=torch.long).unsqueeze(0)\n",
    "        examples.append(torch.cat((question, answer), dim=1).squeeze(dim=0))\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        \n",
    "    # **Pad after concatenation**\n",
    "    examples =tokenizer.pad(\n",
    "        {\"input_ids\": examples},\n",
    "        padding=True,  # Pads to longest sequence in batch\n",
    "        return_tensors=\"pt\"  # Convert to PyTorch tensor\n",
    "    )['input_ids'].to(model.device)\n",
    "\n",
    "    # print(f'Example Shape:{examples.shape}')\n",
    "    \n",
    "    # Forward Pass\n",
    "    outputs = model(examples)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "    # print(f'Logits Shape: {outputs.logits.shape}')\n",
    "    probs = probs[:, :-1, :]\n",
    "    examples = examples[:, 1:]\n",
    "    \n",
    "    # print(f'Example Shape:{examples.shape}')\n",
    "    # print(f'Probs Shape: {probs.shape}')\n",
    "    \n",
    "    gen_probs = torch.gather(probs, 2, examples[:, :, None]).squeeze(-1)\n",
    "    # print(f'GenProbs Shape:{gen_probs.shape}')\n",
    "    \n",
    "    for j in range(examples.shape[0]):\n",
    "        # print(f'Question Shape:{questions[j].shape}')\n",
    "        answer_start_idx = questions[j].shape[1]-1\n",
    "        answer_end_idx = answer_start_idx + answers[j].shape[1]\n",
    "        logprobs=[]\n",
    "        for token, prob in zip(examples[j][answer_start_idx:answer_end_idx], gen_probs[j][answer_start_idx:answer_end_idx]):\n",
    "            logprobs.append(prob.item())\n",
    "            # print(f'{token}:\\t{prob.item()}\\t\\t{torch.exp(torch.tensor(prob.item()))}')\n",
    "        all_outputs[i+j]['teacher_log_probs']=logprobs\n",
    "        all_outputs[i+j]['teacher_correctness']=data_student['score'][i+j]\n",
    "        try:\n",
    "            assert len(all_outputs)==1000\n",
    "        except:\n",
    "            print(i+j)\n",
    "            print(len(all_outputs))\n",
    "    del examples\n",
    "    del outputs\n",
    "    del probs\n",
    "    del gen_probs\n",
    "    del logprobs\n",
    "    del questions\n",
    "    del answers\n",
    "    \n",
    "    gc.collect()  # Trigger Python's garbage collector\n",
    "    torch.cuda.empty_cache()  # Free unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22489a3e-9b00-4ace-88d9-3742aafb2790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c59f6f85-d765-429d-b761-199799109385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f6b153c-7cda-46be-8a31-860bede536cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(all_outputs)):\n",
    "    try:\n",
    "        assert len(all_outputs[i]['student_log_probs'])==len(all_outputs[i]['teacher_log_probs'])\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23abf61f-b126-42d5-95b9-7bb4e4e8515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('logprobs.json', \"w\") as f:\n",
    "    json.dump(all_outputs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9d4ad59-34ca-4286-9634-a43ad779776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'gt_reasoning', 'gt_answer', 'student_token_ids', 'student_reasoning', 'student_answer', 'student_correctness', 'student_log_probs', 'teacher_log_probs', 'teacher_correctness'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fc2eb-eafd-4194-88aa-c78fa914b4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
