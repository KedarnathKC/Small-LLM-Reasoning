{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91722a3d-e845-4788-aa50-80656fcfafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME']=cache_dir\n",
    "os.environ['HF_HUB_CACHE']=cache_dir+'/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1d64fb-6bcb-4738-be5c-f1a299a1e56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c5b53c2-9661-42c4-b0b6-725c8f6b174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "\n",
    "model_name= \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d820cd-28dd-43c2-b5de-3ce355b10bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(ex, prompt_template, task_prompt, few_shot, few_shot_examples, n=3):\n",
    "    prompt=[\n",
    "        {\n",
    "            'role':'system',\n",
    "            'content':prompt_template['system_msg']\n",
    "        }\n",
    "    ]\n",
    "    if few_shot:\n",
    "        for idx in range(n):\n",
    "            prompt.extend([\n",
    "                {\n",
    "                    'role':'user',\n",
    "                    'content':prompt_template['user_msg'].format(instruction=task_prompt, question=few_shot_examples[idx]['input'])\n",
    "                },\n",
    "                {\n",
    "                    'role':'assistant',\n",
    "                    'content':prompt_template['assistant_msg'].format(response=few_shot_examples[idx]['reference'], rationale=few_shot_examples[idx]['rationale'])\n",
    "                }\n",
    "                                                                      \n",
    "            ])\n",
    "        \n",
    "    prompt.append(\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':prompt_template['user_msg'].format(instruction=task_prompt, question=ex)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def tokenize_function(example,input_column, prompt_template, task_prompt, few_shot, few_shot_examples, n=3):\n",
    "    prompt= get_prompt(example[input_column], prompt_template, task_prompt, few_shot, few_shot_examples, n)\n",
    "    prompt= tokenizer.apply_chat_template(prompt,  tokenize= False, add_generation_prompt=True)\n",
    "    return {'input_ids': {'prompt_token_ids':tokenizer(prompt, add_special_tokens=False)['input_ids']}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6dd5192-5f94-4aee-b4d5-5b4f4badfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(task_name, split, input_column, prompt_template, task_prompt, few_shot, few_shot_examples, n):\n",
    "    data_path= f'../datasets/{task_name}'\n",
    "    data = load_from_disk(f\"{data_path}/{split}/\")\n",
    "    tokenized_dataset = data.map(lambda x: tokenize_function(x, input_column, prompt_template, task_prompt, few_shot, few_shot_examples, n), batched=False)\n",
    "    output_path=f\"{data_path}/tokenized/LLaMA70B-Instruct/{split}/{n}-shot/\"\n",
    "    print(output_path)\n",
    "    tokenized_dataset.save_to_disk(output_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39494ecc-0ca0-4442-819a-8f7d88a29f9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2436a780ab416eb5e5a5808d8978a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/gec/tokenized/LLaMA70B-Instruct/val/3-shot/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a6dea37842468794ace69ff04fdece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccf2638b6604522b7483a836253fc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/gec/tokenized/LLaMA70B-Instruct/val/0-shot/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2159e865492d48018f872c03f2265d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5f895178be4e1898af3989fd2214f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/gec/tokenized/LLaMA70B-Instruct/test/3-shot/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0b2719325b40c9bd738e2d6f3e193c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122c47515a7643789fa271b941ee4f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/gec/tokenized/LLaMA70B-Instruct/test/0-shot/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf612d43379478b922dfd0304e20b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_column='input'\n",
    "for task in ['gec']:\n",
    "    task_prompt_path=f'../prompts/{task}.json'\n",
    "    with open(task_prompt_path) as fp:\n",
    "        task_prompt = json.load(fp)\n",
    "    prompt_template={\n",
    "        'system_msg':task_prompt['system_msg'],\n",
    "        'user_msg':task_prompt['user_msg'],\n",
    "        'assistant_msg':task_prompt['assistant_msg']\n",
    "    }\n",
    "    for split in ['val','test',]:\n",
    "    # for split in ['feedback-100','feedback-400','feedback-1600']:\n",
    "        for few_shot in [True,False]:\n",
    "            if few_shot:\n",
    "                for n in [3]:\n",
    "                    tokenize_data(task, split, input_column, prompt_template, task_prompt['task_prompt'], few_shot, task_prompt['few_shot'], n)\n",
    "            else:\n",
    "                tokenize_data(task, split, input_column, prompt_template, task_prompt['task_prompt'], few_shot, task_prompt['few_shot'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f7af5-1e22-4ecb-9577-3f8624a795cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47677021-bbda-4e8b-9a7f-66fde53fd2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98675075-a7bf-4946-aae7-e454123c7041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9033c649-0c4d-476c-bb9f-a6f65b11df46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output'],\n",
       "    num_rows: 1312\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "task='gec'\n",
    "input_column='input'\n",
    "data_path= f'../datasets/{task}'\n",
    "split='test'\n",
    "data = load_from_disk(f\"{data_path}/{split}/\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22fe52a2-f505-4ca0-b553-525e73029412",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prompt_path=f'../prompts/{task}.json'\n",
    "with open(task_prompt_path) as fp:\n",
    "    task_prompt = json.load(fp)\n",
    "\n",
    "prompt_template={\n",
    "    'system_msg':task_prompt['system_msg'],\n",
    "    'user_msg':task_prompt['user_msg'],\n",
    "    'assistant_msg':task_prompt['assistant_msg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4072db4-a016-4351-b013-d41c14da01a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_prompt': 'Correct grammatical errors in the text by first providing a response, followed by an explanation. Please use this template for the explanation: \"The word X should be deleted/inserted/replaced by Y because ...\"',\n",
       " 'task_prompt1': 'Given an input text, the goal is to detect and correct grammatical errors in the text. First explain your reasoning by describing the grammatical errors and how to fix them and then, provide the corrected text.\\n\\nYour response should end with \"The corrected text is: [answer]\" where [answer] is the grammatically correct version of the input text.',\n",
       " 'system_msg': '',\n",
       " 'user_msg': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{question}\\n\\n### Response:\\n',\n",
       " 'assistant_msg': '{rationale} The corrected text is {response}\\n',\n",
       " 'few_shot': [{'id': '8778',\n",
       "   'input': 'way to move from place to another .',\n",
       "   'reference': 'way to move from one place to another .',\n",
       "   'rationale': 'The word \"one\" should be inserted before \"place\" because the phrase \"from place to another\" is ungrammatical. The correct idiomatic expression in English is \"from one place to another,\" which properly matches the singular \"another\" with the singular \"one place\".'},\n",
       "  {'id': '21174',\n",
       "   'input': 'Hi Alison !',\n",
       "   'reference': 'Hi Alison !',\n",
       "   'rationale': 'No grammatical errors are present in the sentence. It is already correct as an informal greeting.'},\n",
       "  {'id': '17866',\n",
       "   'input': 'So , could you tell me what is the best way to reach your house ?',\n",
       "   'reference': 'So , could you tell me which is the best way to reach your house ?',\n",
       "   'rationale': 'The word \"what\" should be replaced by \"which\" because \"which\" is used when selecting from a known or limited set of options, making it more appropriate in this context where the speaker is likely referring to specific possible ways to reach the house.'},\n",
       "  {'id': '19539',\n",
       "   'input': \"What I 'm doing is challenging the way we approach to it .\",\n",
       "   'reference': \"What I 'm doing is challenging the way we approach  it .\",\n",
       "   'rationale': 'The word to should be deleted because the verb approach is a transitive verb and does not require a preposition before its object. Including \"to\" is grammatically incorrect in this context.'},\n",
       "  {'id': '25488',\n",
       "   'input': 'The absence of the parents or the fact they usually have no control nor way to drive their choices makes me very worried about the future of our society .',\n",
       "   'reference': 'The absence of  parents or the fact they usually have no control or way to drive their choices makes me very worried about the future of our society .',\n",
       "   'rationale': 'The word the should be deleted because in this context, \"parents\" is used in a general, plural sense, and does not require the definite article.The word nor should be replaced by or because \"nor\" is typically used in negative constructions following \"neither,\" but here the sentence uses \"no control\" as a standalone negative, making \"or\" the appropriate coordinating conjunction.'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8624078e-b583-4bd8-ac4b-d6068e5c1e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The corrected text is 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template['assistant_msg'].format(response='0',rationale='0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9834196-ea7d-4a93-9d2c-b603efc9dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_column='input'\n",
    "few_shot=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a75eb4c7-5d44-4514-a801-7bca8e6a8003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': ''},\n",
       " {'role': 'user',\n",
       "  'content': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCorrect grammatical errors in the text by first providing a response, followed by an explanation. Please use this template for the explanation: \"The word X should be deleted/inserted/replaced by Y because ...\"\\n\\n### Input:\\nKeeping the Secret of Genetic Testing\\n\\n### Response:\\n'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=get_prompt(data['input'][0], prompt_template, task_prompt['task_prompt'], few_shot, task_prompt['few_shot'], n=2)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e2f5c8-432b-495a-beb3-d7bd98ec1aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Correct grammatical errors in the text by first providing a response, followed by an explanation. Please use this template for the explanation: \"The word X should be deleted/inserted/replaced by Y because ...\"\n",
      "\n",
      "### Input:\n",
      "Keeping the Secret of Genetic Testing\n",
      "\n",
      "### Response:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=tokenizer.apply_chat_template(prompt,  tokenize= False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bffd25b-da34-495b-b8fa-3a30ca70f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "user\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Correct grammatical errors in the text by first providing a response, followed by an explanation. Please use this template for the explanation: \"The word X should be deleted/inserted/replaced by Y because...\"\n",
      "\n",
      "### Input:\n",
      "Keeping the Secret of Genetic Testing\n",
      "\n",
      "### Response:assistant\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=tokenizer.decode(tokenizer(prompt)['input_ids'],skip_special_tokens=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bdbec-8640-4c08-9529-ed3868ed2483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
