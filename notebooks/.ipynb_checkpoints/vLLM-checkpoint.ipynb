{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a94d53-67a4-472a-805a-bb794d1f8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME']=cache_dir\n",
    "os.environ['HF_HUB_CACHE']=cache_dir+'/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e98810f-66cc-4bdf-8d0c-25bcee1ca525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "# import multiprocessing as mp\n",
    "\n",
    "# mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from vllm.inputs import TokensPrompt\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5664f354-d52f-4760-b461-1a8ead0c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'edits', 'output', 'input_ids'],\n",
       "    num_rows: 1312\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data_path= \"../datasets/gec/tokenized/LLaMA3B-Instruct/\"\n",
    "split='test'\n",
    "data=load_from_disk(data_path+split+'/0-shot')\n",
    "# data = load_from_disk(f\"{data_path}/tokenized/LLaMA8B-Instruct/{split}/0-shot/\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b8ec1c-1fd8-4c54-9126-48c2a920ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "\n",
    "model_name= \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68afe37c-2790-4816-97dc-67d2e0947dcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/.no_exist/0cb88a4f764b7a12671c53f0838cd831a0843b95/preprocessor_config.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 23:53:57 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir='/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 05-07 23:53:58 model_runner.py:915] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 05-07 23:53:58 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignored error while writing commit hash to /scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/refs/main: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/refs/main'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c9220e26464ff589ee16464acfc9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 23:53:59 model_runner.py:926] Loading model weights took 6.0160 GB\n",
      "INFO 05-07 23:54:00 gpu_executor.py:122] # GPU blocks: 18966, # CPU blocks: 2340\n",
      "INFO 05-07 23:54:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-07 23:54:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-07 23:54:14 model_runner.py:1335] Graph capturing finished in 11 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1236/1312 [01:31<00:00, 109.36it/s]"
     ]
    }
   ],
   "source": [
    "kwargs={\n",
    "    'download_dir':cache_dir,\n",
    "    'max_model_len': 2000\n",
    "}\n",
    "\n",
    "model = LLM(\n",
    "        model=model_name,  \n",
    "        tensor_parallel_size=1,\n",
    "        trust_remote_code=True,\n",
    "        # gpu_memory_utilization=0.97,  # Increase this value\n",
    "    **kwargs\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3d5da6-71a5-4648-adff-fa593703f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_strings =  ['<|eot_id|>', '<|start_header_id|>user<|end_header_id|>', 'Q:', '</s>', '<|im_end|>']\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 stop=stop_strings,\n",
    "                                 # logprobs=2,\n",
    "                                 best_of=1,\n",
    "                                 presence_penalty=0,\n",
    "                                 top_p=1,\n",
    "                                 top_k=1\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57406379-b68a-4792-b4c7-68672d73c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# task='gec'\n",
    "# task_prompt_path=f'../prompts/{task}.json'\n",
    "# with open(task_prompt_path) as fp:\n",
    "#     task_prompt = json.load(fp)\n",
    "# task_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2e85fb-e2e7-4a58-b434-ada722ce989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=[]\n",
    "# for i in range(len(data)):\n",
    "# # for i in range(26,28):\n",
    "#     chat=[\n",
    "#         {\n",
    "#             \"role\":\"system\",\n",
    "#             \"content\":task_prompt['system_msg'],\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\":\"user\",\n",
    "#             \"content\":task_prompt['user_msg'].format(instruction=task_prompt['task_prompt'], question=data[i]['input'])\n",
    "#         }\n",
    "#     ]\n",
    "#     prompt.append(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True))\n",
    "# # print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68e24bc-f1f4-49b2-8cd2-b843d1b4c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt[332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e7a5ce-ab58-4f06-832b-1d70f8b13231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1312/1312 [00:38<00:00, 33.73it/s, est. speed input: 4611.07 toks/s, output: 3023.16 toks/s] \n"
     ]
    }
   ],
   "source": [
    "all_outputs = model.generate(data['input_ids'], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526fb3d-c950-438f-a433-a6d5e32a83ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
