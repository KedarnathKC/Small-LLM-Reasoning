{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e98810f-66cc-4bdf-8d0c-25bcee1ca525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from vllm.inputs import TokensPrompt\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "from small_llm_reasoning.evaluation.gsm8k import get_score, eight_shot_messages\n",
    "# from small_llm_reasoning.generation.vllm_generation import llama_forward\n",
    "\n",
    "\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/Bidirectional-Teacher-Student-Communication-for-LLM-Alignment/transformers_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5664f354-d52f-4760-b461-1a8ead0c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1319\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data = load_from_disk(\"../datasets/gsm8k/test/\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b8ec1c-1fd8-4c54-9126-48c2a920ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "\n",
    "model_name= \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84191bfa-4a79-4bed-8309-d4fc613246cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 75437.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(ex, few_shot):\n",
    "    prompt = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Given the following problem, reason and give a final answer to the problem.\\nProblem: {ex[\"question\"][i]}\\nYour response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\\n'\n",
    "        }\n",
    "    ]\n",
    "    if few_shot:\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \n",
    "    return tokenizer(examples[\"question\"], add_special_tokens=False)\n",
    "\n",
    "tokenized_dataset = data.map(tokenize_function, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb029977-d606-4306-9b65-f2b4f4b3a688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed9cc9-ef07-4e4c-b667-fb843014feee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f7542-95f3-4b68-b224-feb4edd23642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365ee21-a7bd-454b-bec8-2f79b4e01bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed69ff-9e25-488e-8681-adb76b425757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7628bd-05f8-4a58-a623-286936d92a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acab495b-5f5f-4926-a685-2a1459128420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 12840,    374,    279,   6864,    315,   6890,     30],\n",
       "        [  9241,    374,  22767, 128009, 128009, 128009, 128009]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_output = tokenizer(['what is the capital of India?','Where is Delhi'],  return_tensors='pt', add_special_tokens=False, padding=True)\n",
    "tokenized_output\n",
    "# tokenized_output['input_ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b695c101-bc41-4f80-9117-ca648d3fef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt_token_ids': [12840, 374, 279, 6864, 315, 6890, 30]},\n",
       " {'prompt_token_ids': [9241, 374, 22767, 128009, 128009, 128009, 128009]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TokensPrompt instance\n",
    "tokenized_prompts=[]\n",
    "for input_ids in tokenized_output['input_ids']:\n",
    "    tokenized_prompt = TokensPrompt(prompt_token_ids=input_ids.tolist())\n",
    "    tokenized_prompts.append(tokenized_prompt)\n",
    "tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68afe37c-2790-4816-97dc-67d2e0947dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-24 16:05:54 arg_utils.py:862] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-24 16:05:54 config.py:999] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 02-24 16:05:54 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 02-24 16:05:57 model_runner.py:915] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 02-24 16:05:57 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "INFO 02-24 16:05:57 weight_utils.py:280] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 16:05:57 model_runner.py:926] Loading model weights took 2.3185 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 16:05:58 gpu_executor.py:122] # GPU blocks: 74384, # CPU blocks: 8192\n",
      "INFO 02-24 16:05:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-24 16:05:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-24 16:06:08 model_runner.py:1335] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "# stop_strings =  ['<|eot_id|>', '<|start_header_id|>user<|end_header_id|>', 'Q:', '</s>', '<|im_end|>']\n",
    "\n",
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 # stop=stop_strings,\n",
    "                                 seed=1)\n",
    "\n",
    "model = LLM(\n",
    "        model=model_name, \n",
    "        # tokenizer=model_name, \n",
    "        tensor_parallel_size=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4e7a5ce-ab58-4f06-832b-1d70f8b13231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it, est. speed input: 6.21 toks/s, output: 222.10 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(tokenized_prompts, sampling_params)\n",
    "# outputs = model.generate(['what is the capital of India','Where is Delhi'], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed0f9984-f4f5-4e5b-aafb-89d73c853a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=6, prompt=None, prompt_token_ids=[12840, 374, 279, 6864, 315, 6890, 30], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' New Delhi. The capital of India is New Delhi. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New Delhi is the capital of India. New', token_ids=array('l', [1561, 22767, 13, 578, 6864, 315, 6890, 374, 1561, 22767, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561, 22767, 374, 279, 6864, 315, 6890, 13, 1561]), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740413241.925782, last_token_time=1740413241.925782, first_scheduled_time=1740413241.9301307, first_token_time=1740413241.9511905, time_in_queue=0.0043487548828125, finished_time=1740413244.1816118, scheduler_time=0.01770220510661602, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
      "RequestOutput(request_id=7, prompt=None, prompt_token_ids=[9241, 374, 22767, 128009, 128009, 128009, 128009], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='', token_ids=array('l', [128009]), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1740413241.926398, last_token_time=1740413241.926398, first_scheduled_time=1740413241.9301307, first_token_time=1740413241.9511905, time_in_queue=0.0037326812744140625, finished_time=1740413241.9515314, scheduler_time=0.0005288990214467049, model_forward_time=None, model_execute_time=None), lora_request=None)\n"
     ]
    }
   ],
   "source": [
    "for out in outputs:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c50faaf8-251c-49c5-84ac-d51b3972cf3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllama_forward\u001b[49m(\n\u001b[1;32m      2\u001b[0m         prompts\u001b[38;5;241m=\u001b[39mtokenized_output, \n\u001b[1;32m      3\u001b[0m         model_path\u001b[38;5;241m=\u001b[39mmodel_name, \n\u001b[1;32m      4\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m      5\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m      6\u001b[0m         n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m         n_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llama_forward' is not defined"
     ]
    }
   ],
   "source": [
    "# all_outputs = llama_forward(\n",
    "#         prompts=tokenized_output, \n",
    "#         model_path=model_name, \n",
    "#         max_tokens=256, \n",
    "#         temperature=0, \n",
    "#         n_samples=1,\n",
    "#         n_gpus=1\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f679309-b3f9-4cb2-b5af-6df66164119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Janet has 16 eggs per day. She eats 3 for breakfast, so she has 16 - 3 = 13 eggs left. She bakes muffins for 4, so she has 13 - 4 = 9 eggs left. She sells 9 eggs at $2 each, so she makes 9 x 2 = 18 dollars per day. The final answer is 18\n"
     ]
    }
   ],
   "source": [
    "for ex_outputs in all_outputs:\n",
    "    print(ex_outputs.prompt)\n",
    "    print(ex_outputs.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82969e11-6c15-4a32-9569-f448af340bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-24 16:02:37,552\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 16:02:38 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 02-24 16:02:39 model_runner.py:915] Starting to load model facebook/opt-125m...\n",
      "INFO 02-24 16:02:39 weight_utils.py:236] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.05it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.01it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-24 16:02:40 model_runner.py:926] Loading model weights took 0.2389 GB\n",
      "INFO 02-24 16:02:40 gpu_executor.py:122] # GPU blocks: 71292, # CPU blocks: 7281\n",
      "INFO 02-24 16:02:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-24 16:02:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-24 16:02:49 model_runner.py:1335] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 21.75it/s, est. speed input: 268.33 toks/s, output: 1087.78 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Translate the following English text to French: 'Hello, how are you?'\n",
      "Generated text: \n",
      "\n",
      "It is important to note that we speak French with two genders: on the one hand you have the masculine (Latin for \"fellow\", and the feminine \"other\") and on the other you have the feminine \"speaker\". Therefore,\n",
      "---\n",
      "Input: What is the capital of France?\n",
      "Generated text: \n",
      "The capital of France. It's the second largest country after Belgium and is the second biggest economy in the world. It's also the third largest nation in the world and it's the third largest country in the world.\n",
      "> it's the\n",
      "---\n",
      "Input: Explain the concept of machine learning in simple terms.\n",
      "Generated text: \n",
      "\n",
      "In the past, a scientific exploration of machine learning used to be regarded as an antiquated notion. It was not the first time that a researcher had to explain how an algorithm could be used to generate an algorithm.\n",
      "\n",
      "However, in\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.inputs import TokensPrompt\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "# Define your input strings\n",
    "input_strings = [\n",
    "    \"Translate the following English text to French: 'Hello, how are you?'\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the concept of machine learning in simple terms.\"\n",
    "]\n",
    "\n",
    "# Tokenize the input strings and create TokensPrompt instances\n",
    "tokenized_prompts = [\n",
    "    TokensPrompt(prompt_token_ids=tokenizer.encode(input_string))\n",
    "    for input_string in input_strings\n",
    "]\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=50)\n",
    "\n",
    "# Generate the outputs for the batch\n",
    "outputs = llm.generate(tokenized_prompts, sampling_params)\n",
    "\n",
    "# Print the results\n",
    "for input_string, output in zip(input_strings, outputs):\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Input: {input_string}\")\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a146a5b3-0380-43e4-a830-f5f8a82c03b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt_token_ids': [2,\n",
       "   19163,\n",
       "   19593,\n",
       "   5,\n",
       "   511,\n",
       "   2370,\n",
       "   2788,\n",
       "   7,\n",
       "   1515,\n",
       "   35,\n",
       "   128,\n",
       "   31414,\n",
       "   6,\n",
       "   141,\n",
       "   32,\n",
       "   47,\n",
       "   6600]},\n",
       " {'prompt_token_ids': [2, 2264, 16, 5, 812, 9, 1470, 116]},\n",
       " {'prompt_token_ids': [2,\n",
       "   43043,\n",
       "   1851,\n",
       "   5,\n",
       "   4286,\n",
       "   9,\n",
       "   3563,\n",
       "   2239,\n",
       "   11,\n",
       "   2007,\n",
       "   1110,\n",
       "   4]}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee89d2-a273-4c92-8a7e-bbcb8b65a7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
