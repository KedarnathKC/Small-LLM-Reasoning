{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53d4890-ce60-43d0-b45d-142cf23a39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing conda version miniforge3-24.7.1\n",
      "Loading conda version miniforge3-24.7.1\n"
     ]
    }
   ],
   "source": [
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a94d53-67a4-472a-805a-bb794d1f8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "# cache_dir =  \"/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "# os.environ['HF_HOME']=cache_dir\n",
    "# os.environ['HF_HUB_CACHE']=cache_dir+'/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e98810f-66cc-4bdf-8d0c-25bcee1ca525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/kchimmad_umass_edu/conda_envs/reason/lib/python3.12/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "# import multiprocessing as mp\n",
    "\n",
    "# mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from vllm.inputs import TokensPrompt\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5664f354-d52f-4760-b461-1a8ead0c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids'],\n",
       "    num_rows: 1319\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data_path = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/Small-LLM-Reasoning/datasets/gsm8k/tokenized/Llama-3.2-1B-Instruct/test/0-shot'\n",
    "# data_path= \"../datasets/gsm8k/tokenized/Llama-3.2-1B-Instruct/\"\n",
    "# split='test'\n",
    "# data=load_from_disk(data_path+split+'/0-shot')\n",
    "data=load_from_disk(data_path)\n",
    "# data = load_from_disk(f\"{data_path}/tokenized/LLaMA8B-Instruct/{split}/0-shot/\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b8ec1c-1fd8-4c54-9126-48c2a920ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# # model_name= \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# # model_name='/datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/'\n",
    "# model_name='meta-llama/Llama-3.2-3B-Instruct'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3619403d-97d9-495e-ba67-8f689767c901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68afe37c-2790-4816-97dc-67d2e0947dcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 00:46:59 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir='/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 06-02 00:47:00 model_runner.py:915] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 06-02 00:47:01 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignored error while writing commit hash to /scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/refs/main: [Errno 13] Permission denied: '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/models--meta-llama--Llama-3.2-3B-Instruct/refs/main'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7599ed9a0b45659631a0421268907b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 00:47:12 model_runner.py:926] Loading model weights took 6.0160 GB\n",
      "INFO 06-02 00:47:13 gpu_executor.py:122] # GPU blocks: 18986, # CPU blocks: 2340\n",
      "INFO 06-02 00:47:17 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-02 00:47:17 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-02 00:47:30 model_runner.py:1335] Graph capturing finished in 13 secs.\n"
     ]
    }
   ],
   "source": [
    "kwargs={\n",
    "    'download_dir':cache_dir,\n",
    "    'max_model_len': 2000\n",
    "}\n",
    "\n",
    "model = LLM(\n",
    "        model=model_name,  \n",
    "        tensor_parallel_size=1,\n",
    "        trust_remote_code=True,\n",
    "        # gpu_memory_utilization=0.97,  # Increase this value\n",
    "    **kwargs\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3d5da6-71a5-4648-adff-fa593703f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_strings =  ['<|eot_id|>', '<|start_header_id|>user<|end_header_id|>', 'Q:', '</s>', '<|im_end|>', 'the']\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 stop=stop_strings,\n",
    "                                 # logprobs=2,\n",
    "                                 best_of=1,\n",
    "                                 presence_penalty=0,\n",
    "                                 top_p=1,\n",
    "                                 top_k=1\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57406379-b68a-4792-b4c7-68672d73c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# task='gec'\n",
    "# task_prompt_path=f'../prompts/{task}.json'\n",
    "# with open(task_prompt_path) as fp:\n",
    "#     task_prompt = json.load(fp)\n",
    "# task_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2e85fb-e2e7-4a58-b434-ada722ce989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=[]\n",
    "# for i in range(len(data)):\n",
    "# # for i in range(26,28):\n",
    "#     chat=[\n",
    "#         {\n",
    "#             \"role\":\"system\",\n",
    "#             \"content\":task_prompt['system_msg'],\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\":\"user\",\n",
    "#             \"content\":task_prompt['user_msg'].format(instruction=task_prompt['task_prompt'], question=data[i]['input'])\n",
    "#         }\n",
    "#     ]\n",
    "#     prompt.append(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True))\n",
    "# # print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68e24bc-f1f4-49b2-8cd2-b843d1b4c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt[332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e7a5ce-ab58-4f06-832b-1d70f8b13231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 15.77it/s, est. speed input: 2195.41 toks/s, output: 142.11 toks/s]\n"
     ]
    }
   ],
   "source": [
    "all_outputs = model.generate(data['input_ids'][0], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f526fb3d-c950-438f-a433-a6d5e32a83ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionOutput(index=0, text='To find out how much Janet makes at ', token_ids=array('l', [1271, 1505, 704, 1268, 1790, 54765, 3727, 520, 279]), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=the)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a95c4c7c-4bae-40b5-9bc1-4eb8c7aa8ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find out how much Janet makes at '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbf2b352-f312-4f0e-a25d-8a49b87739f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find out how much Janet makes at the'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(all_outputs[0].outputs[0].token_ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f674e59-53ee-4486-a03e-3cf808930e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find out how much Janet makes at the'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(all_outputs[0].outputs[0].token_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d3f392c-7db0-4a55-984c-76e7b309e047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RequestOutput(request_id=1, prompt=None, prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1187, 13806, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 22818, 279, 2768, 3575, 11, 2944, 323, 3041, 264, 1620, 4320, 311, 279, 3575, 627, 32298, 25, 54765, 753, 78878, 11203, 220, 845, 19335, 824, 1938, 13, 3005, 50777, 2380, 369, 17954, 1475, 6693, 323, 293, 2094, 55404, 1354, 369, 1077, 4885, 1475, 1938, 449, 3116, 13, 3005, 31878, 279, 27410, 520, 279, 20957, 6, 3157, 7446, 369, 400, 17, 824, 7878, 37085, 19151, 13, 2650, 1790, 304, 11441, 1587, 1364, 1304, 1475, 1938, 520, 279, 20957, 6, 3157, 5380, 7927, 2077, 1288, 842, 449, 330, 791, 1620, 4320, 374, 510, 9399, 19727, 1405, 510, 9399, 60, 374, 279, 2077, 311, 279, 3575, 13, 128009, 128006, 78191, 128007, 271], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='To find out how much Janet makes at ', token_ids=array('l', [1271, 1505, 704, 1268, 1790, 54765, 3727, 520, 279]), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=the)], finished=True, metrics=RequestMetrics(arrival_time=1748098541.8771443, last_token_time=1748098541.8771443, first_scheduled_time=1748098541.8791618, first_token_time=1748098541.8899493, time_in_queue=0.002017498016357422, finished_time=1748098541.937178, scheduler_time=0.0005465048598125577, model_forward_time=None, model_execute_time=None), lora_request=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac86311-5b69-4563-af68-71852b768185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
