{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88babd32-577f-469f-889c-240ce1a93a71",
   "metadata": {},
   "source": [
    "#### One of the possible dataset in HF with the name WNC is:https://huggingface.co/datasets/reza-alipour/WNC/tree/main/data\n",
    "\n",
    "#### It is a gated repo, for which i have requested access. But not sure if it is the right one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f6738-6f28-454c-8c47-5a5c37d66f6d",
   "metadata": {},
   "source": [
    "#### So following CoEDIT, I am using there script as is to download and store the dataset \n",
    "\n",
    "https://github.com/facebookresearch/EditEval/blob/main/src/processors/wnc.py\n",
    "\n",
    "\n",
    "https://github.com/facebookresearch/EditEval/blob/main/src/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3267660-f43c-43c5-981e-c6802b238db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac0eb73-7253-4d20-bc4f-d934cad9169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, root, filename=None):\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "    \"\"\"\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(\"Downloading \" + url + \" to \" + fpath)\n",
    "        urllib.request.urlretrieve(url, fpath)\n",
    "    except (urllib.error.URLError, IOError) as e:\n",
    "        if url[:5] == \"https\":\n",
    "            url = url.replace(\"https:\", \"http:\")\n",
    "            print(\"Failed download. Trying https -> http instead.\" \" Downloading \" + url + \" to \" + fpath)\n",
    "            urllib.request.urlretrieve(url, fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c19ffe-6daa-42ec-8e78-5c4c91276d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process(url, download_dir, filename):\n",
    "#     Lets first download the data\n",
    "    if not os.path.exists(download_dir):\n",
    "        download_url(url, download_dir, filename)\n",
    "        os.system(f\"unzip {os.path.join(download_dir, 'bias-corpus')} -d {download_dir}\")\n",
    "        os.system(f\"rm {os.path.join(download_dir, 'bias-corpus')}\")\n",
    "        \n",
    "        \n",
    "    original_columns = [\n",
    "            \"id\",\n",
    "            \"src_tok\",\n",
    "            \"tgt_tok\",\n",
    "            \"input\",\n",
    "            \"edits\",\n",
    "            \"src_POS_tags\",\n",
    "            \"tgt_parse_tags\",\n",
    "        ]\n",
    "\n",
    "    dfs = []\n",
    "    splits= [\"train\", \"dev\", \"test\"]\n",
    "    for split in splits:\n",
    "        full_path = os.path.join(download_dir, \"bias_data\", \"WNC\", \"biased.word.\" + split)\n",
    "        df = pd.read_csv(full_path, sep=\"\\t\", names=original_columns)\n",
    "        df[\"id\"] = df[\"id\"].apply(lambda x: \"wnc-\" + split + \"-\" + str(x))\n",
    "        df[\"wiki_id\"] = df[\"id\"].apply(lambda x: x)\n",
    "        dfs.append(df)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(dfs[0]) \n",
    "    dev_dataset = Dataset.from_pandas(dfs[1])\n",
    "    test_dataset = Dataset.from_pandas(dfs[2])\n",
    "    ds = DatasetDict(\n",
    "        {\n",
    "            \"train\": train_dataset,\n",
    "            \"val\": dev_dataset, \n",
    "            \"test\": test_dataset\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e89683-ae5b-4d23-bb0c-291c82618f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://bit.ly/bias-corpus\"\n",
    "download_dir='./wnc'\n",
    "filename='bias-corpus.zip'\n",
    "ds= download_and_process(url, download_dir, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc08d510-6f18-4fab-a70c-d89a52479573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'src_tok', 'tgt_tok', 'input', 'edits', 'src_POS_tags', 'tgt_parse_tags', 'wiki_id'],\n",
       "        num_rows: 53803\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'src_tok', 'tgt_tok', 'input', 'edits', 'src_POS_tags', 'tgt_parse_tags', 'wiki_id'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'src_tok', 'tgt_tok', 'input', 'edits', 'src_POS_tags', 'tgt_parse_tags', 'wiki_id'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9263fa-8e6e-414a-9b55-4931acea1ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc2536ba0b24c208be31ae69de8c1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/53803 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8784036a2b054e11b26868487b974e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd9b07d07d34aa48229353c716d4e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_path='/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/Small-LLM-Reasoning/datasets/'\n",
    "ds.save_to_disk(dir_path+'neutralization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10c0c1-6fb7-46da-8b9d-d30f5709f7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
