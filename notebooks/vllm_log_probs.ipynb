{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3857964-932c-4a6a-b2fa-c8e8f7e39521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from small_llm_reasoning.evaluation.gsm8k import questions, answers, get_score\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "cache_dir = '/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/transformers_cache/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1feae32-191d-4f0b-9b1d-8a9d61a61313",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= \"../datasets/gsm8k\"\n",
    "data= load_from_disk(f\"{data_path}/tokenized/LLaMA3B/pretrained/test/eight-shot/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb413496-5a28-4b32-b663-624e6f7ac167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tokenizer\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "\n",
    "model_name= \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5007c123-c5ea-4ba4-9a9b-9704cfe5f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-05 21:53:08 arg_utils.py:862] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 03-05 21:53:08 config.py:999] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 03-05 21:53:08 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-3.2-3B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 03-05 21:53:09 model_runner.py:915] Starting to load model meta-llama/Llama-3.2-3B...\n",
      "INFO 03-05 21:53:09 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.49it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 21:53:10 model_runner.py:926] Loading model weights took 5.9847 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 21:53:11 gpu_executor.py:122] # GPU blocks: 19194, # CPU blocks: 2340\n",
      "INFO 03-05 21:53:11 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-05 21:53:11 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-05 21:53:22 model_runner.py:1335] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(n=1,\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500,\n",
    "                                 logprobs=2\n",
    "                                )\n",
    "\n",
    "model = LLM(\n",
    "        model=model_name, \n",
    "        tensor_parallel_size=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b11efb9-3391-4637-be47-200d4c9bf34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['input_ids'][0]['prompt_token_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9682703-eef4-4da2-8c9c-b075f2e9d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4%|▍         | 57/1320 [01:30<33:16,  1.58s/it, est. speed input: 2384.66 toks/s, output: 1126.84 toks/s]\n",
      "Processed prompts:  28%|██▊       | 337/1195 [01:05<02:40,  5.36it/s, est. speed input: 5486.67 toks/s, output: 2589.19 toks/s]"
     ]
    }
   ],
   "source": [
    "outputs = model.generate([data['input_ids'][0]], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668935e8-d06a-42aa-8d10-3838e2234709",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8f3a8-f83e-4521-883c-51a11d37f447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reason",
   "language": "python",
   "name": "reason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
