gpu022

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]

Processing questions:   0%|          | 0/21 [00:00<?, ?it/s]/home/kchimmad_umass_edu/.conda/envs/IESLAnalyse/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/kchimmad_umass_edu/.conda/envs/IESLAnalyse/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(

Processing questions:   5%|▍         | 1/21 [01:15<25:08, 75.40s/it]
Processing questions:  10%|▉         | 2/21 [02:35<24:49, 78.37s/it]
Processing questions:  14%|█▍        | 3/21 [04:11<25:56, 86.47s/it]
Processing questions:  19%|█▉        | 4/21 [06:33<30:42, 108.37s/it]
Processing questions:  24%|██▍       | 5/21 [07:43<25:10, 94.40s/it] 
Processing questions:  29%|██▊       | 6/21 [08:59<22:00, 88.01s/it]
Processing questions:  33%|███▎      | 7/21 [10:21<20:05, 86.13s/it]
Processing questions:  38%|███▊      | 8/21 [12:18<20:47, 95.99s/it]
Processing questions:  43%|████▎     | 9/21 [13:29<17:39, 88.33s/it]
Processing questions:  48%|████▊     | 10/21 [15:51<19:13, 104.88s/it]
Processing questions:  52%|█████▏    | 11/21 [17:11<16:10, 97.07s/it] 
Processing questions:  57%|█████▋    | 12/21 [18:30<13:44, 91.57s/it]
Processing questions:  62%|██████▏   | 13/21 [19:59<12:06, 90.85s/it]
Processing questions:  67%|██████▋   | 14/21 [21:18<10:10, 87.19s/it]
Processing questions:  71%|███████▏  | 15/21 [23:40<10:23, 103.88s/it]
Processing questions:  76%|███████▌  | 16/21 [25:37<08:59, 107.90s/it]
Processing questions:  81%|████████  | 17/21 [27:48<07:39, 114.83s/it]
Processing questions:  86%|████████▌ | 18/21 [29:11<05:15, 105.10s/it]
Processing questions:  90%|█████████ | 19/21 [31:36<03:54, 117.27s/it]
Processing questions:  95%|█████████▌| 20/21 [32:49<01:43, 103.75s/it]
Processing questions: 100%|██████████| 21/21 [33:30<00:00, 85.12s/it] 
Processing questions: 100%|██████████| 21/21 [33:30<00:00, 95.76s/it]
Traceback (most recent call last):
  File "/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/Bidirectional-Teacher-Student-Communication-for-LLM-Alignment/kchimmad/inference3B.py", line 117, in <module>
    score = get_score(data_eval_path,output_file_name)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch3/workspace/wenlongzhao_umass_edu-reason/dev_kedar/Bidirectional-Teacher-Student-Communication-for-LLM-Alignment/kchimmad/score_preds.py", line 39, in get_score
    df.loc[i,'score'] = exact_match_metric.compute(predictions = [df.iloc[i]['model_answer']], references = [test.iloc[i]['GT_Answer']])['exact_match']
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kchimmad_umass_edu/.conda/envs/IESLAnalyse/lib/python3.12/site-packages/evaluate/module.py", line 455, in compute
    self.add_batch(**inputs)
  File "/home/kchimmad_umass_edu/.conda/envs/IESLAnalyse/lib/python3.12/site-packages/evaluate/module.py", line 546, in add_batch
    raise ValueError(error_msg) from None
ValueError: Predictions and/or references don't match the expected format.
Expected format: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},
Input predictions: [None],
Input references: ['8']
