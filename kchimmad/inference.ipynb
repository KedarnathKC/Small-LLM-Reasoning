{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa616fe9-dd17-4171-92f8-39577250fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "os.environ['TRANSFORMERS_CACHE'] = '../transformers_cache/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df455c0-8046-4ec9-ab24-d5e64ec6efd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1319\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = load_from_disk(\"../datasets/gsm8k/test/\")\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d10f80-1a35-4c66-8465-8126e4324405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 896\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = load_from_disk(\"../datasets/gsm8k/train/\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501e21ca-1b79-4430-836b-8ab33eb03ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba31f56cf46646d8aea4f7ecf782107b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_token = os.getenv(\"hf_token\")\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "config = AutoConfig.from_pretrained(model_name, token=hf_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, config=config,cache_dir='../transformers_cache')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token, config=config,cache_dir='../transformers_cache')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73dbc71f-afa2-4e8f-ba7b-19d28a4283b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a09d2e-9097-4336-873e-d4ea29324e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Here your job is to answer a math question. Your question will appear after 8 demonstrations of similar math tasks. As in those demonstrations, you must generate both a step-by-step reasoning and a final answer. Perform a simple action, e.g., a single mathematical operation, at each step of your reasoning. Your final answer must contain only a number and no additional text. State your final answer after ####. \n"
     ]
    }
   ],
   "source": [
    "# 8-shot prompt\n",
    "fewShotPrompt=f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n'\n",
    "'\\n\\nBelow are few example question and answer pairs\\n\\n'\n",
    "\n",
    "fewShotPrompt += \"Here your job is to answer a math question. \"\n",
    "fewShotPrompt += f\"Your question will appear after 8 demonstrations of similar math tasks. \"\n",
    "fewShotPrompt += \"As in those demonstrations, you must generate both a step-by-step reasoning and a final answer. \"\n",
    "fewShotPrompt += \"Perform a simple action, e.g., a single mathematical operation, at each step of your reasoning. \"\n",
    "fewShotPrompt += \"Your final answer must contain only a number and no additional text. \"\n",
    "fewShotPrompt += \"State your final answer after ####. \"\n",
    "\n",
    "print(fewShotPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d23d57-d7f6-4d86-af2e-9fb41b819139",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    fewShotPrompt+=f'Q: {data_train[\"question\"][i]}\\nA: {data_train[\"answer\"][i]}\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc655e9-881e-4b21-864f-6cfd96f0743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/660 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "\n",
      "First, find the number of eggs laid per day that Janet sells at the market: 16 eggs - 3 eggs - 4 eggs = <<16-3-4=9>>9 eggs\n",
      "Then multiply the number of eggs sold by the price per egg: 9 eggs * $2/egg = $<<9*2=18>>18\n",
      "#### 18\n",
      "A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "\n",
      "\n",
      "To find the total number of bolts needed, I need to first find the number of bolts of white fiber. Since it takes half as much white fiber as blue fiber, I will divide the number of blue fiber bolts by 2.\n",
      "\n",
      "2 bolts of blue fiber / 2 = 1 bolt of white fiber\n",
      "\n",
      "Now, I will add the number of bolts of blue fiber and white fiber to find the total number of bolts.\n",
      "\n",
      "2 bolts of blue fiber + 1 bolt of white fiber = 3 bolts\n",
      "\n",
      "#### 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "generated_outputs=[]\n",
    "# Adjust batch size according to your GPU memory capacity\n",
    "# With vram=80G, 1B - 64, 3B - 64, 8B - 32\n",
    "batch_size=2\n",
    "for i in tqdm(range(0, len(data_test[\"question\"]), batch_size), desc=\"Processing questions\"):\n",
    "    batch_questions = data_test[\"question\"][i:i+batch_size]\n",
    "    inputs = [fewShotPrompt+\"Now, solve the below question following the instructions given above. \\n\\nQ: \"+q+\"\\nA: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\" for q in batch_questions]\n",
    "    # inputs = [fewShotPrompt+\"Now, Follow the same format for reasoning and stating your final answer as above examples and Answer the below question\\n\\nQ: \"+q+\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" for q in batch_questions]\n",
    "    tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokenized_inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**tokenized_inputs, max_length=2000, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id, do_sample=True, temperature=0.1, top_p=0.95)\n",
    "        # output = model.generate(**tokenized_inputs, max_new_tokens=256, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id, do_sample=True, temperature=0.1, top_p=0.95)\n",
    "   \n",
    "    for j, o in enumerate(output):\n",
    "        generated_text = tokenizer.decode(o, skip_special_tokens=True)\n",
    "        answer = generated_text.split(\"A: assistant\")[-1]\n",
    "        generated_outputs.append({\"input\": inputs[j], \"output\": generated_text, \"question\": batch_questions[j], \"answer\":answer})\n",
    "        print(batch_questions[j])\n",
    "        print(answer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38503fa8-1984-4920-a877-d9c3bbf85059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IESLAnalyse",
   "language": "python",
   "name": "ieslanalyse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
